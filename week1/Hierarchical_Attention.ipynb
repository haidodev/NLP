{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haidodev/NLP/blob/main/week1/Hierarchical_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a file named glove.6B.zip from the specified URL, which contains pre-trained word embeddings called GloVe embeddings."
      ],
      "metadata": {
        "id": "AUbbP_68Xb-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bifRwnPsgJH",
        "outputId": "7c62f98c-337e-463d-97a6-e0157e9ef00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-21 14:24:32--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-21 14:24:32--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-21 14:24:33--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-04-21 14:27:13 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lyacaimcp9l",
        "outputId": "071dea25-6f37-4453-a6ab-73e44adb0414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aCZHqWRodB5A",
        "outputId": "484a0ad4-af44-4314-99a5-2ccf7c26d4c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLKcSLyndifP"
      },
      "outputs": [],
      "source": [
        "class News20Dataset(Dataset):\n",
        "  def __init__(self, word_map_path, max_sent_length=150, max_doc_length=40, is_train=True):\n",
        "    \"\"\"\n",
        "    Initialize the News20Dataset object.\n",
        "\n",
        "    Args:\n",
        "        word_map_path (str): Path to the word map file.\n",
        "        max_sent_length (int): Maximum length of a sentence.\n",
        "        max_doc_length (int): Maximum length of a document.\n",
        "        is_train (bool): Flag indicating whether the dataset is for training or testing.\n",
        "    \"\"\"\n",
        "    self.max_sent_length = max_sent_length\n",
        "    self.max_doc_length = max_doc_length\n",
        "    self.split = 'train' if is_train else 'test'\n",
        "\n",
        "    # Fetch data from 20 newsgroups dataset\n",
        "    self.data = fetch_20newsgroups(\n",
        "        subset=self.split,\n",
        "        categories=['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'],\n",
        "        shuffle=False,\n",
        "        remove=('headers', 'footers', 'quotes')\n",
        "    )\n",
        "\n",
        "    # Load vocabulary from word map file\n",
        "    self.vocab = pd.read_csv(\n",
        "        filepath_or_buffer=word_map_path,\n",
        "        header=None,\n",
        "        sep=' ',\n",
        "        quoting=csv.QUOTE_NONE,\n",
        "        usecols=[0]\n",
        "    ).values[:50000]\n",
        "\n",
        "    # Create vocabulary list\n",
        "    self.vocab = ['<pad>', '<unk>'] + [word[0] for word in self.vocab]\n",
        "\n",
        "  def transform(self, text):\n",
        "    \"\"\"\n",
        "    Transform text into numerical representation using the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "        list, int, list: Transformed document, number of sentences, number of words per sentence.\n",
        "    \"\"\"\n",
        "    doc = [\n",
        "        [self.vocab.index(word) if word in self.vocab else 1 for word in word_tokenize(text=sent)]\n",
        "        for sent in sent_tokenize(text=text)\n",
        "    ]\n",
        "    doc = [sent[:self.max_sent_length] for sent in doc][:self.max_doc_length]\n",
        "    num_sents = min(len(doc), self.max_doc_length)\n",
        "    if num_sents == 0:\n",
        "      return None, -1, None\n",
        "\n",
        "    num_words = [min(len(sent), self.max_sent_length) for sent in doc][:self.max_doc_length]\n",
        "\n",
        "    return doc, num_sents, num_words\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    label = self.data['target'][i]\n",
        "    text = self.data['data'][i]\n",
        "\n",
        "    doc, num_sents, num_words = self.transform(text)\n",
        "\n",
        "    if num_sents == -1:\n",
        "      return None\n",
        "\n",
        "    return doc, label, num_sents, num_words\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data['data'])\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    \"\"\"\n",
        "    Get the size of the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        int: Size of the vocabulary.\n",
        "    \"\"\"\n",
        "    return len(self.vocab)\n",
        "\n",
        "  @property\n",
        "  def num_classes(self):\n",
        "    \"\"\"\n",
        "    Get the number of classes in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of classes.\n",
        "    \"\"\"\n",
        "    return 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collate function organizes and processes variable-length input data into uniform batches, essential for training neural networks efficiently\n",
        "\n",
        "The below collate function takes a batch of data samples, filters out any None values, organizes them into tensors, and pads them to create uniform batch sizes for efficient processing."
      ],
      "metadata": {
        "id": "ijfMmmCYZYPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pnKdlENh6R_"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  \"\"\"\n",
        "  Collate function for batching data.\n",
        "\n",
        "  Args:\n",
        "      batch (list): List of tuples containing (document, label, number of sentences, number of words per sentence).\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor: Batched tensors for documents, labels, document lengths, and sentence lengths.\n",
        "  \"\"\"\n",
        "  # Filter out None values from the batch\n",
        "  batch = filter(lambda x: x is not None, batch)\n",
        "  # Unzip batch elements\n",
        "  docs, labels, doc_lengths, sent_lengths = list(zip(*batch))\n",
        "\n",
        "  bsz = len(labels)\n",
        "  batch_max_doc_length = max(doc_lengths)\n",
        "  batch_max_sent_length = max([max(sl) if sl else 0 for sl in sent_lengths])\n",
        "\n",
        "  # Initialize tensors for documents and sentence lengths\n",
        "  docs_tensor = torch.zeros([bsz, batch_max_doc_length, batch_max_sent_length]).long()\n",
        "  sent_lengths_tensor = torch.zeros([bsz, batch_max_doc_length]).long()\n",
        "\n",
        "  # Fill in tensors with data from batch\n",
        "  for doc_idx, doc in enumerate(docs):\n",
        "    doc_length = doc_lengths[doc_idx]\n",
        "    sent_lengths_tensor[doc_idx, :doc_length] = torch.LongTensor(sent_lengths[doc_idx])\n",
        "    for sent_idx, sent in enumerate(doc):\n",
        "      sent_length = sent_lengths[doc_idx][sent_idx]\n",
        "      docs_tensor[doc_idx, sent_idx, :sent_length] = torch.LongTensor(sent)\n",
        "\n",
        "  return docs_tensor, torch.LongTensor(labels), torch.LongTensor(doc_lengths), sent_lengths_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataloader"
      ],
      "metadata": {
        "id": "1y8MoymCZnLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTzVZ26l4Dv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnF2SBFdEtMr",
        "outputId": "b194c1c9-654a-4268-feb3-dca31f554220"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mCOhZ2gmCpu"
      },
      "outputs": [],
      "source": [
        "class MyDataLoader(DataLoader):\n",
        "  def __init__(self, dataset, batch_size):\n",
        "    \"\"\"\n",
        "    Custom DataLoader class for handling data loading.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to load.\n",
        "        batch_size (int): Batch size for data loading.\n",
        "    \"\"\"\n",
        "    self.n_samples = len(dataset)\n",
        "    self.sampler = RandomSampler(dataset)\n",
        "\n",
        "    # Define DataLoader initialization arguments\n",
        "    self.init_kwargs = {\n",
        "        'dataset': dataset,\n",
        "        'batch_size': batch_size,\n",
        "        'pin_memory': True,  # Pin memory for faster GPU transfers if available\n",
        "        'collate_fn': collate_fn,  # Collate function for batching data\n",
        "        'shuffle': False  # Disable shuffling to maintain consistency during evaluation\n",
        "    }\n",
        "\n",
        "    # Initialize DataLoader using superclass constructor\n",
        "    super().__init__(sampler=self.sampler, **self.init_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdfSjBtonJkN"
      },
      "outputs": [],
      "source": [
        "dataset = News20Dataset('glove.6B.100d.txt', is_train=True)\n",
        "data_loader = MyDataLoader(dataset, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterates through batches from a data loader, printing information about each batch. It displays the batch index, size, shapes of document and label tensors, document lengths, and shapes of sentence length tensors"
      ],
      "metadata": {
        "id": "r5vLDEgjaGRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD7EPlT9nZ_o",
        "outputId": "c647f132-affc-4ee1-aaf1-76331c191c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0:\n",
            "Batch size: 61\n",
            "Doc tensor shape: torch.Size([61, 40, 150])\n",
            "Docs tensor: tensor([[[    1,     4,     0,  ...,     0,     0,     0],\n",
            "         [ 1115,     1,    40,  ...,     0,     0,     0],\n",
            "         [    1,  1572,   171,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,     3,    24,  ...,     0,     0,     0],\n",
            "         [    1,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,     3,    14,  ...,     0,     0,     0],\n",
            "         [    1,   221,  1411,  ...,     0,     0,     0],\n",
            "         [    1,     3,    20,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    1,   286,    16,  ...,     0,     0,     0],\n",
            "         [    1, 15684,   936,  ...,     0,     0,     0],\n",
            "         [ 2825,   436,  6381,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[   47,     1,  1546,  ...,     0,     0,     0],\n",
            "         [    1,  2717,   157,  ...,     0,     0,     0],\n",
            "         [    1,    37,  3291,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,     2,    30,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]]])\n",
            "Lables tensor shape: torch.Size([61])\n",
            "Document lengths: tensor([ 6,  2, 12,  5,  4,  3,  7,  5,  5,  8,  5,  5,  3,  9,  6,  4, 11, 19,\n",
            "         3,  2,  2,  6,  2,  7,  5, 34, 20, 10,  4, 18,  9,  2,  6,  2,  2, 25,\n",
            "         3,  3,  8,  7,  4, 12,  5,  9, 18, 12, 12, 13, 10,  9,  1,  9, 14,  6,\n",
            "         8, 18,  3, 40, 23,  6,  1])\n",
            "Sentence lengths tensor shape: torch.Size([61, 40])\n",
            "SEntence lengths tensor: tensor([[ 2, 17, 18,  ...,  0,  0,  0],\n",
            "        [27,  1,  0,  ...,  0,  0,  0],\n",
            "        [ 7, 15,  9,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [24, 14, 35,  ...,  0,  0,  0],\n",
            "        [16,  6, 32,  ...,  0,  0,  0],\n",
            "        [15,  0,  0,  ...,  0,  0,  0]])\n",
            "Batch 1:\n",
            "Batch size: 63\n",
            "Doc tensor shape: torch.Size([63, 40, 150])\n",
            "Docs tensor: tensor([[[   1,   68,    5,  ...,    0,    0,    0],\n",
            "         [  67,    1,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
            "\n",
            "        [[   1,  735,    2,  ...,    0,    0,    0],\n",
            "         [   1,   85,  394,  ...,    0,    0,    0],\n",
            "         [   1,   11,   61,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
            "\n",
            "        [[   1,    1,    3,  ...,    0,    0,    0],\n",
            "         [   1, 7363,    4,  ...,    0,    0,    0],\n",
            "         [   1, 4886,    3,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[   1,    1,   25,  ...,    0,    0,    0],\n",
            "         [   1,  431,  127,  ...,    0,    0,    0],\n",
            "         [   1,    2, 1117,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
            "\n",
            "        [[   1,  915, 5138,  ...,    0,    0,    0],\n",
            "         [   1,   85,   55,  ...,    0,    0,    0],\n",
            "         [   1,    2,    1,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
            "\n",
            "        [[  47,    1,   47,  ...,    0,    0,    0],\n",
            "         [   1, 1714,   12,  ...,    0,    0,    0],\n",
            "         [  25,    1,    2,  ...,    0,    0,    0],\n",
            "         ...,\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0],\n",
            "         [   0,    0,    0,  ...,    0,    0,    0]]])\n",
            "Lables tensor shape: torch.Size([63])\n",
            "Document lengths: tensor([ 2,  5,  6, 14, 22, 21,  4,  2,  4, 17, 21,  4, 17,  7, 10,  4,  7, 11,\n",
            "         2,  7, 40,  7,  4,  2, 40,  4,  2,  1,  7,  5, 31,  5, 17, 11,  2,  3,\n",
            "         2,  2,  2, 40,  5, 12,  3, 17,  4, 18,  2,  4, 32,  4,  6, 18,  4,  8,\n",
            "         8,  4, 15,  3, 10,  9, 12,  9,  8])\n",
            "Sentence lengths tensor shape: torch.Size([63, 40])\n",
            "SEntence lengths tensor: tensor([[10,  2,  0,  ...,  0,  0,  0],\n",
            "        [27, 21,  5,  ...,  0,  0,  0],\n",
            "        [35,  3,  9,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [19, 38, 44,  ...,  0,  0,  0],\n",
            "        [46, 51,  7,  ...,  0,  0,  0],\n",
            "        [37, 33, 27,  ...,  0,  0,  0]])\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (docs_tensor, labels, doc_lengths, sent_lengths_tensor) in enumerate(data_loader):\n",
        "  print(f'Batch {batch_idx}:')\n",
        "  print(f'Batch size: {labels.size(0)}')  # Print batch size\n",
        "  print(f'Doc tensor shape: {docs_tensor.shape}')  # Print shape of document tensor\n",
        "  print(f'Docs tensor: {docs_tensor}')  # Print document tensor\n",
        "  print(f'Lables tensor shape: {labels.shape}')  # Print shape of labels tensor\n",
        "  print(f'Document lengths: {doc_lengths}')  # Print document lengths\n",
        "  print(f'Sentence lengths tensor shape: {sent_lengths_tensor.shape}')  # Print shape of sentence length tensor\n",
        "  print(f'Sentence lengths tensor: {sent_lengths_tensor}')  # Print sentence length tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgCr0tkenbBQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Initialize embeddings with pre-trained weights or random initialization.\n",
        "\n",
        "*   Optionally freeze embeddings during training.\n",
        "*   Sort sentences by length for efficient processing.\n",
        "*   Pack sequences for dynamic sequence handling.\n",
        "*   Pass packed sequences through bidirectional GRU layer.\n",
        "*   Optionally apply layer normalization.\n",
        "*   Mục danh sách\n",
        "*   Mục danh sách\n",
        "*   Mục danh sách\n",
        "*   Mục danh sách\n",
        "*   Mục danh sách\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Compute attention weights and normalize them.\n",
        "Apply attention to GRU outputs and aggregate attended representations.\n",
        "Return sentence embeddings and attention weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "o-O90tnIabEQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB2n7VKgtwsP"
      },
      "outputs": [],
      "source": [
        "class WordAttention(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, gru_hidden_dim, gru_num_layers, att_dim, use_layer_norm, dropout):\n",
        "    \"\"\"\n",
        "    Initialize the WordAttention module.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_dim (int): Dimension of word embeddings.\n",
        "        gru_hidden_dim (int): Dimension of GRU hidden states.\n",
        "        gru_num_layers (int): Number of layers in the GRU.\n",
        "        att_dim (int): Dimension of attention vectors.\n",
        "        use_layer_norm (bool): Whether to use layer normalization.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    super(WordAttention, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.gru = nn.GRU(\n",
        "        embed_dim,\n",
        "        gru_hidden_dim,\n",
        "        num_layers=gru_num_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.use_layer_norm = use_layer_norm\n",
        "    if use_layer_norm:\n",
        "      self.layer_norm = nn.LayerNorm(2 * gru_hidden_dim, elementwise_affine=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.attention = nn.Linear(2 * gru_hidden_dim, att_dim)\n",
        "\n",
        "    self.context_vector = nn.Linear(att_dim, 1, bias=False)\n",
        "\n",
        "  def init_embeddings(self, embeddings):\n",
        "    \"\"\"\n",
        "    Initialize the embedding layer with pre-trained embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings (torch.Tensor): Pre-trained word embeddings.\n",
        "    \"\"\"\n",
        "    self.embeddings.weight = nn.Parameter(embeddings)\n",
        "\n",
        "  def freeze_embeddings(self, freeze=False):\n",
        "    \"\"\"\n",
        "    Freeze or unfreeze the embedding layer.\n",
        "\n",
        "    Args:\n",
        "        freeze (bool): Whether to freeze the embedding layer.\n",
        "    \"\"\"\n",
        "    self.embeddings.weight.requires_grad = not freeze\n",
        "\n",
        "  def forward(self, sents, sent_lengths):\n",
        "    \"\"\"\n",
        "    Forward pass of the WordAttention module.\n",
        "\n",
        "    Args:\n",
        "        sents (torch.Tensor): Input sentences.\n",
        "        sent_lengths (torch.Tensor): Lengths of input sentences.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor, torch.Tensor: Sentence embeddings and attention weights.\n",
        "    \"\"\"\n",
        "    # Sort sentences by length for efficient processing\n",
        "    sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
        "    sents = sents[sent_perm_idx]\n",
        "\n",
        "    # Pass sentences through embedding layer and apply dropout\n",
        "    sents = self.embeddings(sents)\n",
        "    sents = self.dropout(sents)\n",
        "\n",
        "    # Pack sequences for dynamic sequence handling\n",
        "    packed_words = pack_padded_sequence(sents, lengths=sent_lengths.tolist(), batch_first=True)\n",
        "\n",
        "    valid_bsz = packed_words.batch_sizes\n",
        "\n",
        "    # Pass packed sequences through bidirectional GRU layer\n",
        "    packed_words, _ = self.gru(packed_words)\n",
        "\n",
        "    # Optionally apply layer normalization\n",
        "    if self.use_layer_norm:\n",
        "      normed_words = self.layer_norm(packed_words.data)\n",
        "    else:\n",
        "      normed_words = packed_words\n",
        "\n",
        "    # Compute attention weights\n",
        "    att = torch.tanh(self.attention(normed_words.data))\n",
        "    att = self.context_vector(att).squeeze(1)\n",
        "    val = att.max()\n",
        "    att = torch.exp(att - val)\n",
        "    att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
        "    att_weights = att / torch.sum(att, dim = 1, keepdim=True)\n",
        "\n",
        "    # Apply attention to GRU outputs and aggregate attended representations\n",
        "    sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
        "    sents = sents * att_weights.unsqueeze(2)\n",
        "    sents = sents.sum(dim=1)\n",
        "\n",
        "    # Restore original order of sentences\n",
        "    _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
        "    sents = sents[sent_unperm_idx]\n",
        "    att_weights = att_weights[sent_unperm_idx]\n",
        "\n",
        "    return sents, att_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Sorting: Documents are sorted by decreasing order of length for efficient processing.\n",
        "*   Word-level Attention: Each document's sentences are passed through the word-level attention module to obtain word-level attention weights and sentence embeddings.\n",
        "*   Dropout: Optionally, dropout is applied to the sentence embeddings.\n",
        "*   Packing: Sentences are packed into a long batch by removing pad-sentences for dynamic sequence handling.\n",
        "*   Sentence-level GRU: The packed sentence embeddings are passed through a bidirectional sentence-level GRU to capture contextual information.\n",
        "*   Layer Normalization (Optional): If enabled, layer normalization is applied to the sentence embeddings.\n",
        "*   Sentence-level Attention: The sentence embeddings are passed through a linear layer followed by a context vector to compute sentence-level attention weights.\n",
        "*   Normalization: Attention weights are normalized.\n",
        "*   Repadding: Documents are restored by repadding the packed sentence embeddings.\n",
        "*   Document Computation: Document vectors are computed by weighting the sentence embeddings with the sentence-level attention weights and summing them.\n",
        "*   Finalization: Attention weights of words and sentences are restored to their original order."
      ],
      "metadata": {
        "id": "RdPMWI-xbWDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcuvPbTsxVom"
      },
      "outputs": [],
      "source": [
        "class SentenceAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Sentence-level attention module with a word-level attention module.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
        "              word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
        "      \"\"\"\n",
        "      Initialize the SentenceAttention module.\n",
        "\n",
        "      Args:\n",
        "          vocab_size (int): Size of the vocabulary.\n",
        "          embed_dim (int): Dimension of word embeddings.\n",
        "          word_gru_hidden_dim (int): Dimension of word-level GRU hidden states.\n",
        "          sent_gru_hidden_dim (int): Dimension of sentence-level GRU hidden states.\n",
        "          word_gru_num_layers (int): Number of layers in the word-level GRU.\n",
        "          sent_gru_num_layers (int): Number of layers in the sentence-level GRU.\n",
        "          word_att_dim (int): Dimension of word-level attention vectors.\n",
        "          sent_att_dim (int): Dimension of sentence-level attention vectors.\n",
        "          use_layer_norm (bool): Whether to use layer normalization.\n",
        "          dropout (float): Dropout probability.\n",
        "      \"\"\"\n",
        "      super(SentenceAttention, self).__init__()\n",
        "\n",
        "      # Word-level attention module\n",
        "      self.word_attention = WordAttention(vocab_size, embed_dim, word_gru_hidden_dim, word_gru_num_layers,\n",
        "                                          word_att_dim, use_layer_norm, dropout)\n",
        "\n",
        "      # Bidirectional sentence-level GRU\n",
        "      self.gru = nn.GRU(2 * word_gru_hidden_dim, sent_gru_hidden_dim, num_layers=sent_gru_num_layers,\n",
        "                        batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "      # Optionally apply layer normalization\n",
        "      self.use_layer_norm = use_layer_norm\n",
        "      if use_layer_norm:\n",
        "          self.layer_norm = nn.LayerNorm(2 * sent_gru_hidden_dim, elementwise_affine=True)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      # Sentence-level attention\n",
        "      self.sent_attention = nn.Linear(2 * sent_gru_hidden_dim, sent_att_dim)\n",
        "\n",
        "      # Sentence context vector u_s to take dot product with\n",
        "      self.sentence_context_vector = nn.Linear(sent_att_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, docs, doc_lengths, sent_lengths):\n",
        "      \"\"\"\n",
        "      Forward pass of the SentenceAttention module.\n",
        "\n",
        "      Args:\n",
        "          docs (torch.Tensor): Encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length).\n",
        "          doc_lengths (torch.Tensor): Unpadded document lengths; LongTensor (num_docs).\n",
        "          sent_lengths (torch.Tensor): Unpadded sentence lengths; LongTensor (num_docs, padded_doc_length).\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor, torch.Tensor, torch.Tensor: Document embeddings, attention weights of words, attention weights of sentences.\n",
        "      \"\"\"\n",
        "      # Sort documents by decreasing order in length\n",
        "      doc_lengths, doc_perm_idx = doc_lengths.sort(dim=0, descending=True)\n",
        "      docs = docs[doc_perm_idx]\n",
        "      sent_lengths = sent_lengths[doc_perm_idx]\n",
        "\n",
        "      # Make a long batch of sentences by removing pad-sentences\n",
        "      packed_sents = pack_padded_sequence(docs, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "      # effective batch size at each timestep\n",
        "      valid_bsz = packed_sents.batch_sizes\n",
        "\n",
        "      # Make a long batch of sentence lengths by removing pad-sentences\n",
        "      packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "      # Word attention module\n",
        "      sents, word_att_weights = self.word_attention(packed_sents.data, packed_sent_lengths.data)\n",
        "\n",
        "      # Optionally apply dropout\n",
        "      sents = self.dropout(sents)\n",
        "\n",
        "      # Sentence-level GRU over sentence embeddings\n",
        "      packed_sents, _ = self.gru(PackedSequence(sents, valid_bsz))\n",
        "\n",
        "      # Optionally apply layer normalization\n",
        "      if self.use_layer_norm:\n",
        "          normed_sents = self.layer_norm(packed_sents.data)\n",
        "      else:\n",
        "          normed_sents = packed_sents\n",
        "\n",
        "      # Sentence attention\n",
        "      att = torch.tanh(self.sent_attention(normed_sents))\n",
        "      att = self.sentence_context_vector(att).squeeze(1)\n",
        "\n",
        "      # Normalize attention weights\n",
        "      val = att.max()\n",
        "      att = torch.exp(att - val)\n",
        "      att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
        "      sent_att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
        "\n",
        "      # Restore as documents by repadding\n",
        "      docs, _ = pad_packed_sequence(packed_sents, batch_first=True)\n",
        "\n",
        "      # Compute document vectors\n",
        "      docs = docs * sent_att_weights.unsqueeze(2)\n",
        "      docs = docs.sum(dim=1)\n",
        "\n",
        "      # Restore as documents by repadding\n",
        "      word_att_weights, _ = pad_packed_sequence(PackedSequence(word_att_weights, valid_bsz), batch_first=True)\n",
        "\n",
        "      # Restore the original order of documents (undo the first sorting)\n",
        "      _, doc_unperm_idx = doc_perm_idx.sort(dim=0, descending=False)\n",
        "      docs = docs[doc_unperm_idx]\n",
        "      word_att_weights = word_att_weights[doc_unperm_idx]\n",
        "      sent_att_weights = sent_att_weights[doc_unperm_idx]\n",
        "\n",
        "      return docs, word_att_weights, sent_att_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Initialization: The constructor initializes the model with parameters such as the number of classes, vocabulary size, embedding dimensions, GRU hidden dimensions, number of layers, attention dimensions, layer normalization usage, and dropout probability.\n",
        "*   Sentence Attention Module: It contains an instance of the SentenceAttention module, which is responsible for computing document embeddings and attention weights at both word and sentence levels.\n",
        "*   Fully Connected Layer: The model has a fully connected layer (fc) for classification, which takes the document embeddings as input and produces class scores.\n",
        "*   Forward Pass: In the forward method, input documents along with their lengths (both document lengths and sentence lengths) are passed through the SentenceAttention module to compute document embeddings (doc_embed) and attention weights (word_att_weights and sent_att_weights). Then, the document embeddings are fed into the fully connected layer to obtain class scores (scores).\n",
        "*   Return: The method returns the class scores along with the word-level and sentence-level attention weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "QJz4AFQvcJkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8yGC3_LCD0k"
      },
      "outputs": [],
      "source": [
        "class HierarchicalAttentionNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  Hierarchical Attention Network for document classification.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               num_classes,\n",
        "               vocab_size,\n",
        "               embed_dim,\n",
        "               word_gru_hidden_dim,\n",
        "               sent_gru_hidden_dim,\n",
        "               word_gru_num_layers,\n",
        "               sent_gru_num_layers,\n",
        "               word_att_dim,\n",
        "               sent_att_dim,\n",
        "               use_layer_norm,\n",
        "               dropout):\n",
        "    \"\"\"\n",
        "    Initialize the HierarchicalAttentionNetwork module.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of classes for classification.\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_dim (int): Dimension of word embeddings.\n",
        "        word_gru_hidden_dim (int): Dimension of word-level GRU hidden states.\n",
        "        sent_gru_hidden_dim (int): Dimension of sentence-level GRU hidden states.\n",
        "        word_gru_num_layers (int): Number of layers in the word-level GRU.\n",
        "        sent_gru_num_layers (int): Number of layers in the sentence-level GRU.\n",
        "        word_att_dim (int): Dimension of word-level attention vectors.\n",
        "        sent_att_dim (int): Dimension of sentence-level attention vectors.\n",
        "        use_layer_norm (bool): Whether to use layer normalization.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    super(HierarchicalAttentionNetwork, self).__init__()\n",
        "    self.sent_attention = SentenceAttention(\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        word_gru_hidden_dim,\n",
        "        sent_gru_hidden_dim,\n",
        "        word_gru_num_layers,\n",
        "        sent_gru_num_layers,\n",
        "        word_att_dim,\n",
        "        sent_att_dim,\n",
        "        use_layer_norm,\n",
        "        dropout\n",
        "    )\n",
        "\n",
        "    # Fully connected layer for classification\n",
        "    self.fc = nn.Linear(2 * sent_gru_hidden_dim, num_classes)\n",
        "\n",
        "    self.use_layer_norm = use_layer_norm\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, docs, doc_lengths, sent_lengths):\n",
        "    \"\"\"\n",
        "    Forward pass of the HierarchicalAttentionNetwork module.\n",
        "\n",
        "    Args:\n",
        "        docs (torch.Tensor): Encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length).\n",
        "        doc_lengths (torch.Tensor): Unpadded document lengths; LongTensor (num_docs).\n",
        "        sent_lengths (torch.Tensor): Unpadded sentence lengths; LongTensor (num_docs, padded_doc_length).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor, torch.Tensor, torch.Tensor: Scores, word-level attention weights, sentence-level attention weights.\n",
        "    \"\"\"\n",
        "    # Compute document embeddings and attention weights\n",
        "    doc_embed, word_att_weights, sent_att_weights = self.sent_attention(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "    # Pass document embeddings through the fully connected layer for classification\n",
        "    scores = self.fc(doc_embed)\n",
        "\n",
        "    return scores, word_att_weights, sent_att_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpVdrrGxylA2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pylab import *\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u5LHKlJy6zf"
      },
      "outputs": [],
      "source": [
        "class MetricTracker(object):\n",
        "  \"\"\"\n",
        "  Class to track and compute metrics during training or evaluation.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize the MetricTracker.\n",
        "    \"\"\"\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset all metrics to zero.\n",
        "    \"\"\"\n",
        "    self.val = 0\n",
        "    self.avg = 0\n",
        "    self.sum = 0\n",
        "    self.count = 0\n",
        "\n",
        "  def update(self, summed_val, n=1):\n",
        "    \"\"\"\n",
        "    Update metrics based on new values.\n",
        "\n",
        "    Args:\n",
        "        summed_val (float): Summed value of the metric over a batch or epoch.\n",
        "        n (int): Number of samples in the batch or epoch.\n",
        "    \"\"\"\n",
        "    # Compute average value for the current batch or epoch\n",
        "    self.val = summed_val / n\n",
        "    # Update sum of all values encountered so far\n",
        "    self.sum += summed_val\n",
        "    # Increment count of total number of samples seen\n",
        "    self.count += n\n",
        "    # Compute running average of all values encountered so far\n",
        "    self.avg = self.sum / self.count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Method: The eval method performs the evaluation process. It sets the model to evaluation mode (eval()) and disables gradient calculation (torch.no_grad()). It resets the accuracy tracker (accs) and iterates over batches of data from the evaluation data loader. For each batch, it moves the data to the appropriate device, computes scores using the model, calculates predictions, and updates the accuracy tracker with the number of correct predictions. After iterating over all batches, it prints the average accuracy on the test set and updates the best_acc attribute if the current average accuracy is higher than the previous best accuracy.\n"
      ],
      "metadata": {
        "id": "rWuhvbsDcehc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLVat94S_y1_"
      },
      "outputs": [],
      "source": [
        "class Evaluation:\n",
        "  \"\"\"\n",
        "  Class to evaluate a model on a dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, config, model):\n",
        "    \"\"\"\n",
        "    Initialize the Evaluation instance.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration dictionary.\n",
        "        model: Trained model for evaluation.\n",
        "    \"\"\"\n",
        "    self.config = config\n",
        "    self.model = model\n",
        "    self.device = next(self.model.parameters()).device\n",
        "\n",
        "    # Initialize dataset and data loader for evaluation\n",
        "    self.dataset = News20Dataset(config['vocab_path'], is_train=False)\n",
        "    self.dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Initialize accuracy tracker\n",
        "    self.accs = MetricTracker()\n",
        "    self.best_acc = 0\n",
        "\n",
        "  def eval(self):\n",
        "    \"\"\"\n",
        "    Perform evaluation on the dataset.\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    self.model.eval()\n",
        "    # Disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "      # Reset accuracy tracker\n",
        "      self.accs.reset()\n",
        "\n",
        "      # Iterate over batches of data\n",
        "      for (docs, labels, doc_lengths, sent_lengths) in self.dataloader:\n",
        "        batch_size = labels.size(0)\n",
        "\n",
        "        # Move data to device\n",
        "        docs = docs.to(self.device)\n",
        "        sent_lengths = sent_lengths.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        doc_lengths = doc_lengths.to(self.device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        scores, word_at_weights, sentence_att_weights = self.model(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "        # Compute predictions\n",
        "        predictions = scores.max(dim=1)[1]\n",
        "\n",
        "        # Calculate accuracy for the batch\n",
        "        correct_predictions = torch.eq(predictions, labels).sum().item()\n",
        "        acc = correct_predictions\n",
        "\n",
        "        # Update accuracy tracker\n",
        "        self.accs.update(acc, batch_size)\n",
        "\n",
        "      # Update best accuracy if current average accuracy is higher\n",
        "      self.best_acc = max(self.best_acc, self.accs.avg)\n",
        "\n",
        "      # Print test average accuracy\n",
        "      print('Test Average Accuracy: {acc.avg:.4f}'.format(acc=self.accs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Training Method: The train method performs the training process. It iterates over epochs specified in the configuration. For each epoch, it calls the _train_epoch method to train the model for one epoch. After each epoch, it prints the average loss and accuracy for that epoch. It then evaluates the model using the Evaluation class (tester) and saves the model if the current accuracy is the best so far.\n",
        "*   Epoch Training Method: The _train_epoch method trains the model for one epoch. It sets the model to training mode, resets the loss and accuracy trackers, and iterates over batches of training data. For each batch, it moves the data to the appropriate device, performs a forward pass through the model to compute scores, computes the loss, performs backpropagation, and updates model parameters using the optimizer. It also calculates accuracy for the batch and updates the loss and accuracy trackers. Additionally, it prints the loss and accuracy for each batch during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "9ipOaMi-cxx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByAiBvFQEvS8"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  \"\"\"\n",
        "  Class to train a model using specified optimizer and criterion.\n",
        "  \"\"\"\n",
        "  def __init__(self, config, model, optimizer, criterion, dataloader):\n",
        "    \"\"\"\n",
        "    Initialize the Trainer.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration dictionary.\n",
        "        model: Model to train.\n",
        "        optimizer: Optimizer for updating model parameters.\n",
        "        criterion: Criterion (loss function) for training.\n",
        "        dataloader: DataLoader for training data.\n",
        "    \"\"\"\n",
        "    self.config = config\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.criterion = criterion\n",
        "    self.dataloader = dataloader\n",
        "    self.device = next(self.model.parameters()).device\n",
        "\n",
        "    # Initialize metric trackers for loss and accuracy\n",
        "    self.losses = MetricTracker()\n",
        "    self.accs = MetricTracker()\n",
        "\n",
        "    # Initialize evaluation instance for testing\n",
        "    self.tester = Evaluation(self.config, self.model)\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(self.config['num_epochs']):\n",
        "      # Train for one epoch\n",
        "      result = self._train_epoch(epoch)\n",
        "      # Print epoch-level results\n",
        "      print('Epoch: [{0}]\\t Avg Loss {loss:.4f}\\t Avg Accuracy {acc:.3f}'.format(epoch, loss=result['loss'], acc=result['acc']))\n",
        "\n",
        "      # Evaluate the model\n",
        "      self.tester.eval()\n",
        "      # Save the model if the current accuracy is the best so far\n",
        "      if self.tester.best_acc == self.tester.accs.avg:\n",
        "        print('Saving Model...')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': self.model,\n",
        "            'optimizer': self.optimizer\n",
        "        }, 'best_model/model.pth.tar')\n",
        "\n",
        "  def _train_epoch(self, epoch_idx):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        epoch_idx (int): Index of the current epoch.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing average loss and accuracy for the epoch.\n",
        "    \"\"\"\n",
        "    # Set model to training mode\n",
        "    self.model.train()\n",
        "\n",
        "    # Reset loss and accuracy trackers\n",
        "    self.losses.reset()\n",
        "    self.accs.reset()\n",
        "\n",
        "    # Iterate over batches of training data\n",
        "    for batch_idx, (docs, labels, doc_lengths, sent_lengths) in enumerate(self.dataloader):\n",
        "        batch_size = labels.size(0)\n",
        "\n",
        "        # Move data to device\n",
        "        docs = docs.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        sent_lengths = sent_lengths.to(self.device)\n",
        "        doc_lengths = doc_lengths.to(self.device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        scores, word_att_weights, sentence_att_weights = self.model(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = self.criterion(scores, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients if specified\n",
        "        if self.config['max_grad_norm'] is not None:\n",
        "          torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])\n",
        "\n",
        "        # Update model parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Compute accuracy for the batch\n",
        "        predictions = scores.max(dim=1)[1]\n",
        "        correct_predictions = torch.eq(predictions, labels).sum().item()\n",
        "        acc = correct_predictions\n",
        "\n",
        "        # Update loss and accuracy trackers\n",
        "        self.losses.update(loss.item(), batch_size)\n",
        "        self.accs.update(acc, batch_size)\n",
        "\n",
        "        # Print batch-level results\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t Loss {loss.val:.4f} ({loss.avg:.4f})\\t Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(epoch_idx, batch_idx, len(self.dataloader), loss=self.losses, acc=self.accs))\n",
        "\n",
        "    # Return epoch-level results\n",
        "    log = {\n",
        "        'loss': self.losses.avg,\n",
        "        'acc': self.accs.avg\n",
        "    }\n",
        "    return log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   Dataset and DataLoader Setup: It initializes the training dataset using News20Dataset and creates a DataLoader (dataloader) using the custom DataLoader class MyDataLoader.\n",
        "*   Model Initialization: It initializes the model (HierarchicalAttentionNetwork) with the specified parameters and moves it to the specified device.\n",
        "Optimizer Initialization: It initializes the optimizer (Adam) for updating model parameters.\n",
        "*   Loss Function Initialization: It initializes the criterion (CrossEntropyLoss) for computing the loss during training.\n",
        "*   Pretrained Embeddings Initialization (Optional): If pretrain is set to True in the configuration, it retrieves pretrained embeddings (glove_pretrained) using the dataset's vocabulary and initializes the model's embeddings with these pretrained weights. It also freezes the embeddings if specified in the configuration.\n",
        "*   Trainer Initialization: It initializes the Trainer instance with the configured model, optimizer, criterion, and dataloader.\n",
        "*   Training: It starts the training process by calling the train method of the Trainer instance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XZhnQmdGdIYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL7DbmpQCW2E"
      },
      "outputs": [],
      "source": [
        "def train(config, device):\n",
        "  \"\"\"\n",
        "  Train a hierarchical attention network model.\n",
        "\n",
        "  Args:\n",
        "      config (dict): Configuration dictionary containing hyperparameters and settings.\n",
        "      device: Device to use for training (e.g., 'cuda' for GPU or 'cpu').\n",
        "\n",
        "  \"\"\"\n",
        "  # Initialize training dataset and dataloader\n",
        "  dataset = News20Dataset(config['vocab_path'], is_train=True)\n",
        "  dataloader = MyDataLoader(dataset, batch_size=config['batch_size'])\n",
        "\n",
        "  # Initialize the model\n",
        "  model = HierarchicalATtentionNetwork(\n",
        "        num_classes=dataset.num_classes,\n",
        "        vocab_size=dataset.vocab_size,\n",
        "        embed_dim=config['embed_dim'],\n",
        "        word_gru_hidden_dim=config['word_gru_hidden_dim'],\n",
        "        sent_gru_hidden_dim=config['sent_gru_hidden_dim'],\n",
        "        word_gru_num_layers=config['word_gru_num_layers'],\n",
        "        sent_gru_num_layers=config['sent_gru_num_layers'],\n",
        "        word_att_dim=config['word_att_dim'],\n",
        "        sent_att_dim=config['sent_att_dim'],\n",
        "        use_layer_norm=config['use_layer_norm'],\n",
        "        dropout=config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "  # Initialize optimizer\n",
        "  optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=config['lr'])\n",
        "\n",
        "  # Initialize loss function\n",
        "  criterion = nn.CrossEntropyLoss(reduction='sum').to(device)\n",
        "\n",
        "  # Initialize pretrained embeddings if specified\n",
        "  if config['pretrain']:\n",
        "    glove_pretrained = get_pretrained_weights(dataset.vocab, config['embed_dim'], device)\n",
        "    model.sent_attention.word_attention.init_embeddings(glove_pretrained)\n",
        "\n",
        "  # Freeze embeddings if specified\n",
        "  model.sent_attention.word_attention.freeze_embeddings(config['freeze'])\n",
        "\n",
        "  # Initialize trainer\n",
        "  trainer = Trainer(config, model, optimizer, criterion, dataloader)\n",
        "\n",
        "  # Start training\n",
        "  trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Zz68q9CZkb"
      },
      "outputs": [],
      "source": [
        "os.makedirs('best_model', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNOc9pNLCaQ8",
        "outputId": "f9ee834d-4c6d-4d95-ed1d-fd6d5f290ee5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/38]\t Loss 1.3773 (1.3773)\t Accuracy 0.349 (0.349)\n",
            "Epoch: [0][1/38]\t Loss 1.6421 (1.5108)\t Accuracy 0.188 (0.268)\n",
            "Epoch: [0][2/38]\t Loss 1.5068 (1.5094)\t Accuracy 0.286 (0.274)\n",
            "Epoch: [0][3/38]\t Loss 1.3631 (1.4730)\t Accuracy 0.381 (0.300)\n",
            "Epoch: [0][4/38]\t Loss 1.3521 (1.4489)\t Accuracy 0.333 (0.307)\n",
            "Epoch: [0][5/38]\t Loss 1.3215 (1.4280)\t Accuracy 0.323 (0.310)\n",
            "Epoch: [0][6/38]\t Loss 1.3207 (1.4137)\t Accuracy 0.500 (0.335)\n",
            "Epoch: [0][7/38]\t Loss 1.2724 (1.3961)\t Accuracy 0.468 (0.351)\n",
            "Epoch: [0][8/38]\t Loss 1.3341 (1.3892)\t Accuracy 0.365 (0.353)\n",
            "Epoch: [0][9/38]\t Loss 1.2489 (1.3750)\t Accuracy 0.397 (0.357)\n",
            "Epoch: [0][10/38]\t Loss 1.1936 (1.3584)\t Accuracy 0.492 (0.370)\n",
            "Epoch: [0][11/38]\t Loss 1.2182 (1.3468)\t Accuracy 0.468 (0.378)\n",
            "Epoch: [0][12/38]\t Loss 1.0642 (1.3252)\t Accuracy 0.581 (0.393)\n",
            "Epoch: [0][13/38]\t Loss 0.9875 (1.3008)\t Accuracy 0.635 (0.411)\n",
            "Epoch: [0][14/38]\t Loss 1.0421 (1.2837)\t Accuracy 0.548 (0.420)\n",
            "Epoch: [0][15/38]\t Loss 0.9477 (1.2631)\t Accuracy 0.574 (0.429)\n",
            "Epoch: [0][16/38]\t Loss 0.8752 (1.2401)\t Accuracy 0.603 (0.440)\n",
            "Epoch: [0][17/38]\t Loss 0.9941 (1.2263)\t Accuracy 0.683 (0.453)\n",
            "Epoch: [0][18/38]\t Loss 0.9405 (1.2118)\t Accuracy 0.583 (0.460)\n",
            "Epoch: [0][19/38]\t Loss 0.8915 (1.1958)\t Accuracy 0.694 (0.471)\n",
            "Epoch: [0][20/38]\t Loss 0.8719 (1.1807)\t Accuracy 0.689 (0.482)\n",
            "Epoch: [0][21/38]\t Loss 0.8121 (1.1640)\t Accuracy 0.726 (0.493)\n",
            "Epoch: [0][22/38]\t Loss 0.8514 (1.1505)\t Accuracy 0.613 (0.498)\n",
            "Epoch: [0][23/38]\t Loss 0.6796 (1.1309)\t Accuracy 0.758 (0.509)\n",
            "Epoch: [0][24/38]\t Loss 0.8933 (1.1211)\t Accuracy 0.672 (0.515)\n",
            "Epoch: [0][25/38]\t Loss 0.7030 (1.1048)\t Accuracy 0.683 (0.522)\n",
            "Epoch: [0][26/38]\t Loss 0.8701 (1.0962)\t Accuracy 0.661 (0.527)\n",
            "Epoch: [0][27/38]\t Loss 0.7460 (1.0839)\t Accuracy 0.656 (0.532)\n",
            "Epoch: [0][28/38]\t Loss 0.6043 (1.0672)\t Accuracy 0.730 (0.539)\n",
            "Epoch: [0][29/38]\t Loss 0.4622 (1.0468)\t Accuracy 0.825 (0.548)\n",
            "Epoch: [0][30/38]\t Loss 0.7596 (1.0376)\t Accuracy 0.677 (0.552)\n",
            "Epoch: [0][31/38]\t Loss 0.9442 (1.0346)\t Accuracy 0.524 (0.551)\n",
            "Epoch: [0][32/38]\t Loss 0.8523 (1.0289)\t Accuracy 0.688 (0.556)\n",
            "Epoch: [0][33/38]\t Loss 0.5731 (1.0158)\t Accuracy 0.770 (0.562)\n",
            "Epoch: [0][34/38]\t Loss 0.7593 (1.0085)\t Accuracy 0.710 (0.566)\n",
            "Epoch: [0][35/38]\t Loss 0.7609 (1.0014)\t Accuracy 0.625 (0.568)\n",
            "Epoch: [0][36/38]\t Loss 0.6149 (0.9909)\t Accuracy 0.810 (0.574)\n",
            "Epoch: [0][37/38]\t Loss 0.3033 (0.9894)\t Accuracy 1.000 (0.575)\n",
            "Epoch: [0]\t Avg Loss 0.9894\t Avg Accuracy 0.575\n",
            "Test Average Accuracy: 0.7072\n",
            "Saving Model...\n",
            "Epoch: [1][0/38]\t Loss 0.4909 (0.4909)\t Accuracy 0.790 (0.790)\n",
            "Epoch: [1][1/38]\t Loss 0.5523 (0.5216)\t Accuracy 0.758 (0.774)\n",
            "Epoch: [1][2/38]\t Loss 0.5694 (0.5377)\t Accuracy 0.778 (0.775)\n",
            "Epoch: [1][3/38]\t Loss 0.5696 (0.5456)\t Accuracy 0.774 (0.775)\n",
            "Epoch: [1][4/38]\t Loss 0.5420 (0.5449)\t Accuracy 0.794 (0.779)\n",
            "Epoch: [1][5/38]\t Loss 0.5594 (0.5473)\t Accuracy 0.787 (0.780)\n",
            "Epoch: [1][6/38]\t Loss 0.5591 (0.5490)\t Accuracy 0.746 (0.775)\n",
            "Epoch: [1][7/38]\t Loss 0.4980 (0.5427)\t Accuracy 0.770 (0.775)\n",
            "Epoch: [1][8/38]\t Loss 0.8758 (0.5791)\t Accuracy 0.656 (0.762)\n",
            "Epoch: [1][9/38]\t Loss 0.5240 (0.5737)\t Accuracy 0.787 (0.764)\n",
            "Epoch: [1][10/38]\t Loss 0.6885 (0.5845)\t Accuracy 0.781 (0.766)\n",
            "Epoch: [1][11/38]\t Loss 0.5704 (0.5833)\t Accuracy 0.762 (0.765)\n",
            "Epoch: [1][12/38]\t Loss 0.5499 (0.5806)\t Accuracy 0.797 (0.768)\n",
            "Epoch: [1][13/38]\t Loss 0.4811 (0.5736)\t Accuracy 0.806 (0.771)\n",
            "Epoch: [1][14/38]\t Loss 0.7927 (0.5883)\t Accuracy 0.683 (0.765)\n",
            "Epoch: [1][15/38]\t Loss 0.7625 (0.5992)\t Accuracy 0.742 (0.763)\n",
            "Epoch: [1][16/38]\t Loss 0.8159 (0.6115)\t Accuracy 0.683 (0.759)\n",
            "Epoch: [1][17/38]\t Loss 0.6233 (0.6121)\t Accuracy 0.787 (0.760)\n",
            "Epoch: [1][18/38]\t Loss 0.5173 (0.6072)\t Accuracy 0.770 (0.761)\n",
            "Epoch: [1][19/38]\t Loss 0.3929 (0.5963)\t Accuracy 0.825 (0.764)\n",
            "Epoch: [1][20/38]\t Loss 0.7424 (0.6034)\t Accuracy 0.778 (0.765)\n",
            "Epoch: [1][21/38]\t Loss 0.6644 (0.6062)\t Accuracy 0.750 (0.764)\n",
            "Epoch: [1][22/38]\t Loss 0.6649 (0.6087)\t Accuracy 0.754 (0.764)\n",
            "Epoch: [1][23/38]\t Loss 0.6522 (0.6104)\t Accuracy 0.776 (0.764)\n",
            "Epoch: [1][24/38]\t Loss 0.8646 (0.6208)\t Accuracy 0.587 (0.757)\n",
            "Epoch: [1][25/38]\t Loss 0.7128 (0.6243)\t Accuracy 0.730 (0.756)\n",
            "Epoch: [1][26/38]\t Loss 0.8314 (0.6322)\t Accuracy 0.672 (0.753)\n",
            "Epoch: [1][27/38]\t Loss 0.6339 (0.6323)\t Accuracy 0.790 (0.754)\n",
            "Epoch: [1][28/38]\t Loss 0.6316 (0.6323)\t Accuracy 0.750 (0.754)\n",
            "Epoch: [1][29/38]\t Loss 0.5283 (0.6288)\t Accuracy 0.823 (0.756)\n",
            "Epoch: [1][30/38]\t Loss 0.3653 (0.6201)\t Accuracy 0.891 (0.761)\n",
            "Epoch: [1][31/38]\t Loss 0.5452 (0.6178)\t Accuracy 0.836 (0.763)\n",
            "Epoch: [1][32/38]\t Loss 0.6096 (0.6175)\t Accuracy 0.742 (0.762)\n",
            "Epoch: [1][33/38]\t Loss 0.5677 (0.6161)\t Accuracy 0.746 (0.762)\n",
            "Epoch: [1][34/38]\t Loss 0.6324 (0.6165)\t Accuracy 0.734 (0.761)\n",
            "Epoch: [1][35/38]\t Loss 0.5024 (0.6133)\t Accuracy 0.797 (0.762)\n",
            "Epoch: [1][36/38]\t Loss 0.6602 (0.6146)\t Accuracy 0.766 (0.762)\n",
            "Epoch: [1][37/38]\t Loss 0.2441 (0.6140)\t Accuracy 1.000 (0.763)\n",
            "Epoch: [1]\t Avg Loss 0.6140\t Avg Accuracy 0.763\n",
            "Test Average Accuracy: 0.7170\n",
            "Saving Model...\n",
            "Epoch: [2][0/38]\t Loss 0.5006 (0.5006)\t Accuracy 0.841 (0.841)\n",
            "Epoch: [2][1/38]\t Loss 0.5940 (0.5470)\t Accuracy 0.774 (0.808)\n",
            "Epoch: [2][2/38]\t Loss 0.6736 (0.5889)\t Accuracy 0.694 (0.770)\n",
            "Epoch: [2][3/38]\t Loss 0.5874 (0.5885)\t Accuracy 0.742 (0.763)\n",
            "Epoch: [2][4/38]\t Loss 0.4768 (0.5660)\t Accuracy 0.794 (0.769)\n",
            "Epoch: [2][5/38]\t Loss 0.3608 (0.5324)\t Accuracy 0.885 (0.788)\n",
            "Epoch: [2][6/38]\t Loss 0.6542 (0.5500)\t Accuracy 0.746 (0.782)\n",
            "Epoch: [2][7/38]\t Loss 0.7051 (0.5690)\t Accuracy 0.672 (0.769)\n",
            "Epoch: [2][8/38]\t Loss 0.6200 (0.5747)\t Accuracy 0.758 (0.767)\n",
            "Epoch: [2][9/38]\t Loss 0.5808 (0.5753)\t Accuracy 0.778 (0.768)\n",
            "Epoch: [2][10/38]\t Loss 0.4984 (0.5684)\t Accuracy 0.787 (0.770)\n",
            "Epoch: [2][11/38]\t Loss 0.3950 (0.5546)\t Accuracy 0.814 (0.774)\n",
            "Epoch: [2][12/38]\t Loss 0.5822 (0.5568)\t Accuracy 0.806 (0.776)\n",
            "Epoch: [2][13/38]\t Loss 0.4325 (0.5480)\t Accuracy 0.820 (0.779)\n",
            "Epoch: [2][14/38]\t Loss 0.2849 (0.5304)\t Accuracy 0.887 (0.786)\n",
            "Epoch: [2][15/38]\t Loss 0.5974 (0.5347)\t Accuracy 0.778 (0.786)\n",
            "Epoch: [2][16/38]\t Loss 0.7735 (0.5492)\t Accuracy 0.734 (0.783)\n",
            "Epoch: [2][17/38]\t Loss 0.4590 (0.5442)\t Accuracy 0.790 (0.783)\n",
            "Epoch: [2][18/38]\t Loss 0.4299 (0.5382)\t Accuracy 0.869 (0.788)\n",
            "Epoch: [2][19/38]\t Loss 0.5233 (0.5375)\t Accuracy 0.855 (0.791)\n",
            "Epoch: [2][20/38]\t Loss 0.2772 (0.5249)\t Accuracy 0.905 (0.796)\n",
            "Epoch: [2][21/38]\t Loss 0.4772 (0.5227)\t Accuracy 0.810 (0.797)\n",
            "Epoch: [2][22/38]\t Loss 0.5057 (0.5220)\t Accuracy 0.778 (0.796)\n",
            "Epoch: [2][23/38]\t Loss 0.6188 (0.5260)\t Accuracy 0.714 (0.793)\n",
            "Epoch: [2][24/38]\t Loss 0.5240 (0.5260)\t Accuracy 0.825 (0.794)\n",
            "Epoch: [2][25/38]\t Loss 0.5837 (0.5282)\t Accuracy 0.766 (0.793)\n",
            "Epoch: [2][26/38]\t Loss 0.6288 (0.5320)\t Accuracy 0.714 (0.790)\n",
            "Epoch: [2][27/38]\t Loss 0.6001 (0.5345)\t Accuracy 0.828 (0.791)\n",
            "Epoch: [2][28/38]\t Loss 0.4622 (0.5321)\t Accuracy 0.831 (0.793)\n",
            "Epoch: [2][29/38]\t Loss 0.3663 (0.5266)\t Accuracy 0.857 (0.795)\n",
            "Epoch: [2][30/38]\t Loss 0.4653 (0.5246)\t Accuracy 0.839 (0.796)\n",
            "Epoch: [2][31/38]\t Loss 0.3235 (0.5183)\t Accuracy 0.887 (0.799)\n",
            "Epoch: [2][32/38]\t Loss 0.4608 (0.5166)\t Accuracy 0.825 (0.800)\n",
            "Epoch: [2][33/38]\t Loss 0.4107 (0.5134)\t Accuracy 0.841 (0.801)\n",
            "Epoch: [2][34/38]\t Loss 0.3242 (0.5079)\t Accuracy 0.859 (0.803)\n",
            "Epoch: [2][35/38]\t Loss 0.4098 (0.5051)\t Accuracy 0.871 (0.805)\n",
            "Epoch: [2][36/38]\t Loss 0.4585 (0.5038)\t Accuracy 0.812 (0.805)\n",
            "Epoch: [2][37/38]\t Loss 1.3759 (0.5057)\t Accuracy 0.400 (0.804)\n",
            "Epoch: [2]\t Avg Loss 0.5057\t Avg Accuracy 0.804\n",
            "Test Average Accuracy: 0.7754\n",
            "Saving Model...\n",
            "Epoch: [3][0/38]\t Loss 0.3970 (0.3970)\t Accuracy 0.869 (0.869)\n",
            "Epoch: [3][1/38]\t Loss 0.3171 (0.3564)\t Accuracy 0.905 (0.887)\n",
            "Epoch: [3][2/38]\t Loss 0.3562 (0.3563)\t Accuracy 0.857 (0.877)\n",
            "Epoch: [3][3/38]\t Loss 0.7062 (0.4445)\t Accuracy 0.698 (0.832)\n",
            "Epoch: [3][4/38]\t Loss 0.3167 (0.4198)\t Accuracy 0.883 (0.842)\n",
            "Epoch: [3][5/38]\t Loss 0.4108 (0.4183)\t Accuracy 0.825 (0.839)\n",
            "Epoch: [3][6/38]\t Loss 0.5355 (0.4350)\t Accuracy 0.806 (0.834)\n",
            "Epoch: [3][7/38]\t Loss 0.3461 (0.4239)\t Accuracy 0.855 (0.837)\n",
            "Epoch: [3][8/38]\t Loss 0.3388 (0.4147)\t Accuracy 0.933 (0.847)\n",
            "Epoch: [3][9/38]\t Loss 0.4387 (0.4171)\t Accuracy 0.803 (0.843)\n",
            "Epoch: [3][10/38]\t Loss 0.3094 (0.4071)\t Accuracy 0.857 (0.844)\n",
            "Epoch: [3][11/38]\t Loss 0.3896 (0.4056)\t Accuracy 0.859 (0.846)\n",
            "Epoch: [3][12/38]\t Loss 0.4449 (0.4087)\t Accuracy 0.810 (0.843)\n",
            "Epoch: [3][13/38]\t Loss 0.5148 (0.4165)\t Accuracy 0.797 (0.839)\n",
            "Epoch: [3][14/38]\t Loss 0.3779 (0.4139)\t Accuracy 0.905 (0.844)\n",
            "Epoch: [3][15/38]\t Loss 0.4384 (0.4154)\t Accuracy 0.820 (0.842)\n",
            "Epoch: [3][16/38]\t Loss 0.4571 (0.4179)\t Accuracy 0.794 (0.839)\n",
            "Epoch: [3][17/38]\t Loss 0.4508 (0.4197)\t Accuracy 0.825 (0.839)\n",
            "Epoch: [3][18/38]\t Loss 0.3153 (0.4141)\t Accuracy 0.891 (0.841)\n",
            "Epoch: [3][19/38]\t Loss 0.3843 (0.4126)\t Accuracy 0.839 (0.841)\n",
            "Epoch: [3][20/38]\t Loss 0.3702 (0.4105)\t Accuracy 0.859 (0.842)\n",
            "Epoch: [3][21/38]\t Loss 0.3854 (0.4094)\t Accuracy 0.855 (0.843)\n",
            "Epoch: [3][22/38]\t Loss 0.5234 (0.4143)\t Accuracy 0.823 (0.842)\n",
            "Epoch: [3][23/38]\t Loss 0.6554 (0.4244)\t Accuracy 0.714 (0.837)\n",
            "Epoch: [3][24/38]\t Loss 0.5715 (0.4305)\t Accuracy 0.797 (0.835)\n",
            "Epoch: [3][25/38]\t Loss 0.7916 (0.4438)\t Accuracy 0.667 (0.829)\n",
            "Epoch: [3][26/38]\t Loss 0.3889 (0.4417)\t Accuracy 0.859 (0.830)\n",
            "Epoch: [3][27/38]\t Loss 0.4190 (0.4409)\t Accuracy 0.857 (0.831)\n",
            "Epoch: [3][28/38]\t Loss 0.2288 (0.4338)\t Accuracy 0.934 (0.834)\n",
            "Epoch: [3][29/38]\t Loss 0.2907 (0.4290)\t Accuracy 0.887 (0.836)\n",
            "Epoch: [3][30/38]\t Loss 0.4067 (0.4283)\t Accuracy 0.839 (0.836)\n",
            "Epoch: [3][31/38]\t Loss 0.3833 (0.4269)\t Accuracy 0.873 (0.837)\n",
            "Epoch: [3][32/38]\t Loss 0.3223 (0.4238)\t Accuracy 0.869 (0.838)\n",
            "Epoch: [3][33/38]\t Loss 0.3365 (0.4212)\t Accuracy 0.905 (0.840)\n",
            "Epoch: [3][34/38]\t Loss 0.4995 (0.4235)\t Accuracy 0.825 (0.840)\n",
            "Epoch: [3][35/38]\t Loss 0.6440 (0.4295)\t Accuracy 0.787 (0.838)\n",
            "Epoch: [3][36/38]\t Loss 0.4564 (0.4302)\t Accuracy 0.871 (0.839)\n",
            "Epoch: [3][37/38]\t Loss 1.2605 (0.4316)\t Accuracy 0.500 (0.839)\n",
            "Epoch: [3]\t Avg Loss 0.4316\t Avg Accuracy 0.839\n",
            "Test Average Accuracy: 0.7669\n",
            "Epoch: [4][0/38]\t Loss 0.2517 (0.2517)\t Accuracy 0.917 (0.917)\n",
            "Epoch: [4][1/38]\t Loss 0.5346 (0.3966)\t Accuracy 0.794 (0.854)\n",
            "Epoch: [4][2/38]\t Loss 0.7656 (0.5229)\t Accuracy 0.734 (0.813)\n",
            "Epoch: [4][3/38]\t Loss 0.4491 (0.5043)\t Accuracy 0.825 (0.816)\n",
            "Epoch: [4][4/38]\t Loss 0.4004 (0.4839)\t Accuracy 0.836 (0.820)\n",
            "Epoch: [4][5/38]\t Loss 0.3681 (0.4654)\t Accuracy 0.864 (0.827)\n",
            "Epoch: [4][6/38]\t Loss 0.2497 (0.4349)\t Accuracy 0.869 (0.833)\n",
            "Epoch: [4][7/38]\t Loss 0.3070 (0.4186)\t Accuracy 0.873 (0.838)\n",
            "Epoch: [4][8/38]\t Loss 0.3897 (0.4153)\t Accuracy 0.889 (0.844)\n",
            "Epoch: [4][9/38]\t Loss 0.3631 (0.4100)\t Accuracy 0.857 (0.845)\n",
            "Epoch: [4][10/38]\t Loss 0.5065 (0.4191)\t Accuracy 0.781 (0.839)\n",
            "Epoch: [4][11/38]\t Loss 0.3235 (0.4110)\t Accuracy 0.889 (0.843)\n",
            "Epoch: [4][12/38]\t Loss 0.5300 (0.4201)\t Accuracy 0.806 (0.841)\n",
            "Epoch: [4][13/38]\t Loss 0.3895 (0.4179)\t Accuracy 0.855 (0.842)\n",
            "Epoch: [4][14/38]\t Loss 0.3564 (0.4138)\t Accuracy 0.873 (0.844)\n",
            "Epoch: [4][15/38]\t Loss 0.3477 (0.4097)\t Accuracy 0.902 (0.847)\n",
            "Epoch: [4][16/38]\t Loss 0.4259 (0.4107)\t Accuracy 0.839 (0.847)\n",
            "Epoch: [4][17/38]\t Loss 0.3262 (0.4059)\t Accuracy 0.906 (0.850)\n",
            "Epoch: [4][18/38]\t Loss 0.3917 (0.4051)\t Accuracy 0.847 (0.850)\n",
            "Epoch: [4][19/38]\t Loss 0.4214 (0.4060)\t Accuracy 0.828 (0.849)\n",
            "Epoch: [4][20/38]\t Loss 0.4349 (0.4073)\t Accuracy 0.800 (0.847)\n",
            "Epoch: [4][21/38]\t Loss 0.3252 (0.4035)\t Accuracy 0.905 (0.849)\n",
            "Epoch: [4][22/38]\t Loss 0.3964 (0.4032)\t Accuracy 0.889 (0.851)\n",
            "Epoch: [4][23/38]\t Loss 0.4798 (0.4065)\t Accuracy 0.797 (0.849)\n",
            "Epoch: [4][24/38]\t Loss 0.3572 (0.4045)\t Accuracy 0.905 (0.851)\n",
            "Epoch: [4][25/38]\t Loss 0.5366 (0.4095)\t Accuracy 0.738 (0.847)\n",
            "Epoch: [4][26/38]\t Loss 0.4583 (0.4113)\t Accuracy 0.836 (0.846)\n",
            "Epoch: [4][27/38]\t Loss 0.3114 (0.4076)\t Accuracy 0.905 (0.848)\n",
            "Epoch: [4][28/38]\t Loss 0.3551 (0.4058)\t Accuracy 0.873 (0.849)\n",
            "Epoch: [4][29/38]\t Loss 0.2963 (0.4021)\t Accuracy 0.889 (0.851)\n",
            "Epoch: [4][30/38]\t Loss 0.4401 (0.4034)\t Accuracy 0.859 (0.851)\n",
            "Epoch: [4][31/38]\t Loss 0.3934 (0.4031)\t Accuracy 0.841 (0.851)\n",
            "Epoch: [4][32/38]\t Loss 0.3021 (0.4000)\t Accuracy 0.855 (0.851)\n",
            "Epoch: [4][33/38]\t Loss 0.4992 (0.4029)\t Accuracy 0.803 (0.849)\n",
            "Epoch: [4][34/38]\t Loss 0.4254 (0.4035)\t Accuracy 0.855 (0.850)\n",
            "Epoch: [4][35/38]\t Loss 0.3583 (0.4022)\t Accuracy 0.841 (0.849)\n",
            "Epoch: [4][36/38]\t Loss 0.3246 (0.4001)\t Accuracy 0.844 (0.849)\n",
            "Epoch: [4][37/38]\t Loss 0.6919 (0.4007)\t Accuracy 0.600 (0.849)\n",
            "Epoch: [4]\t Avg Loss 0.4007\t Avg Accuracy 0.849\n",
            "Test Average Accuracy: 0.7814\n",
            "Saving Model...\n",
            "Epoch: [5][0/38]\t Loss 0.3370 (0.3370)\t Accuracy 0.857 (0.857)\n",
            "Epoch: [5][1/38]\t Loss 0.3302 (0.3336)\t Accuracy 0.859 (0.858)\n",
            "Epoch: [5][2/38]\t Loss 0.3207 (0.3294)\t Accuracy 0.867 (0.861)\n",
            "Epoch: [5][3/38]\t Loss 0.4823 (0.3675)\t Accuracy 0.790 (0.843)\n",
            "Epoch: [5][4/38]\t Loss 0.4668 (0.3870)\t Accuracy 0.803 (0.835)\n",
            "Epoch: [5][5/38]\t Loss 0.3779 (0.3855)\t Accuracy 0.873 (0.842)\n",
            "Epoch: [5][6/38]\t Loss 0.4292 (0.3917)\t Accuracy 0.871 (0.846)\n",
            "Epoch: [5][7/38]\t Loss 0.2595 (0.3757)\t Accuracy 0.900 (0.853)\n",
            "Epoch: [5][8/38]\t Loss 0.2575 (0.3631)\t Accuracy 0.898 (0.857)\n",
            "Epoch: [5][9/38]\t Loss 0.3460 (0.3614)\t Accuracy 0.855 (0.857)\n",
            "Epoch: [5][10/38]\t Loss 0.3525 (0.3605)\t Accuracy 0.859 (0.857)\n",
            "Epoch: [5][11/38]\t Loss 0.5930 (0.3803)\t Accuracy 0.762 (0.849)\n",
            "Epoch: [5][12/38]\t Loss 0.4374 (0.3847)\t Accuracy 0.919 (0.855)\n",
            "Epoch: [5][13/38]\t Loss 0.2450 (0.3747)\t Accuracy 0.919 (0.859)\n",
            "Epoch: [5][14/38]\t Loss 0.2882 (0.3688)\t Accuracy 0.873 (0.860)\n",
            "Epoch: [5][15/38]\t Loss 0.1638 (0.3558)\t Accuracy 0.921 (0.864)\n",
            "Epoch: [5][16/38]\t Loss 0.4373 (0.3607)\t Accuracy 0.794 (0.860)\n",
            "Epoch: [5][17/38]\t Loss 0.4303 (0.3645)\t Accuracy 0.820 (0.858)\n",
            "Epoch: [5][18/38]\t Loss 0.3459 (0.3635)\t Accuracy 0.875 (0.859)\n",
            "Epoch: [5][19/38]\t Loss 0.3746 (0.3640)\t Accuracy 0.885 (0.860)\n",
            "Epoch: [5][20/38]\t Loss 0.3426 (0.3630)\t Accuracy 0.857 (0.860)\n",
            "Epoch: [5][21/38]\t Loss 0.3068 (0.3605)\t Accuracy 0.883 (0.861)\n",
            "Epoch: [5][22/38]\t Loss 0.3896 (0.3618)\t Accuracy 0.828 (0.859)\n",
            "Epoch: [5][23/38]\t Loss 0.3274 (0.3604)\t Accuracy 0.902 (0.861)\n",
            "Epoch: [5][24/38]\t Loss 0.4050 (0.3622)\t Accuracy 0.839 (0.860)\n",
            "Epoch: [5][25/38]\t Loss 0.4279 (0.3647)\t Accuracy 0.810 (0.858)\n",
            "Epoch: [5][26/38]\t Loss 0.3442 (0.3640)\t Accuracy 0.873 (0.859)\n",
            "Epoch: [5][27/38]\t Loss 0.2851 (0.3611)\t Accuracy 0.875 (0.859)\n",
            "Epoch: [5][28/38]\t Loss 0.4385 (0.3638)\t Accuracy 0.844 (0.859)\n",
            "Epoch: [5][29/38]\t Loss 0.4332 (0.3661)\t Accuracy 0.823 (0.858)\n",
            "Epoch: [5][30/38]\t Loss 0.3690 (0.3662)\t Accuracy 0.855 (0.858)\n",
            "Epoch: [5][31/38]\t Loss 0.2129 (0.3614)\t Accuracy 0.937 (0.860)\n",
            "Epoch: [5][32/38]\t Loss 0.2301 (0.3574)\t Accuracy 0.935 (0.862)\n",
            "Epoch: [5][33/38]\t Loss 0.3193 (0.3563)\t Accuracy 0.873 (0.863)\n",
            "Epoch: [5][34/38]\t Loss 0.2215 (0.3523)\t Accuracy 0.938 (0.865)\n",
            "Epoch: [5][35/38]\t Loss 0.3982 (0.3536)\t Accuracy 0.875 (0.865)\n",
            "Epoch: [5][36/38]\t Loss 0.3390 (0.3532)\t Accuracy 0.852 (0.865)\n",
            "Epoch: [5][37/38]\t Loss 0.7273 (0.3540)\t Accuracy 0.600 (0.864)\n",
            "Epoch: [5]\t Avg Loss 0.3540\t Avg Accuracy 0.864\n",
            "Test Average Accuracy: 0.7617\n",
            "Epoch: [6][0/38]\t Loss 0.2108 (0.2108)\t Accuracy 0.902 (0.902)\n",
            "Epoch: [6][1/38]\t Loss 0.3167 (0.2628)\t Accuracy 0.898 (0.900)\n",
            "Epoch: [6][2/38]\t Loss 0.4076 (0.3132)\t Accuracy 0.844 (0.880)\n",
            "Epoch: [6][3/38]\t Loss 0.3104 (0.3125)\t Accuracy 0.905 (0.887)\n",
            "Epoch: [6][4/38]\t Loss 0.1574 (0.2817)\t Accuracy 0.951 (0.899)\n",
            "Epoch: [6][5/38]\t Loss 0.3889 (0.2995)\t Accuracy 0.836 (0.889)\n",
            "Epoch: [6][6/38]\t Loss 0.1789 (0.2821)\t Accuracy 0.935 (0.896)\n",
            "Epoch: [6][7/38]\t Loss 0.3816 (0.2946)\t Accuracy 0.823 (0.886)\n",
            "Epoch: [6][8/38]\t Loss 0.2968 (0.2949)\t Accuracy 0.871 (0.885)\n",
            "Epoch: [6][9/38]\t Loss 0.3631 (0.3018)\t Accuracy 0.873 (0.883)\n",
            "Epoch: [6][10/38]\t Loss 0.2675 (0.2987)\t Accuracy 0.869 (0.882)\n",
            "Epoch: [6][11/38]\t Loss 0.4225 (0.3091)\t Accuracy 0.855 (0.880)\n",
            "Epoch: [6][12/38]\t Loss 0.2594 (0.3052)\t Accuracy 0.889 (0.881)\n",
            "Epoch: [6][13/38]\t Loss 0.3405 (0.3078)\t Accuracy 0.841 (0.878)\n",
            "Epoch: [6][14/38]\t Loss 0.3862 (0.3131)\t Accuracy 0.841 (0.875)\n",
            "Epoch: [6][15/38]\t Loss 0.2472 (0.3089)\t Accuracy 0.921 (0.878)\n",
            "Epoch: [6][16/38]\t Loss 0.3505 (0.3114)\t Accuracy 0.873 (0.878)\n",
            "Epoch: [6][17/38]\t Loss 0.2919 (0.3103)\t Accuracy 0.889 (0.878)\n",
            "Epoch: [6][18/38]\t Loss 0.4284 (0.3165)\t Accuracy 0.823 (0.876)\n",
            "Epoch: [6][19/38]\t Loss 0.4734 (0.3243)\t Accuracy 0.823 (0.873)\n",
            "Epoch: [6][20/38]\t Loss 0.1635 (0.3167)\t Accuracy 0.952 (0.877)\n",
            "Epoch: [6][21/38]\t Loss 0.2134 (0.3121)\t Accuracy 0.902 (0.878)\n",
            "Epoch: [6][22/38]\t Loss 0.1959 (0.3069)\t Accuracy 0.937 (0.880)\n",
            "Epoch: [6][23/38]\t Loss 0.3669 (0.3095)\t Accuracy 0.889 (0.881)\n",
            "Epoch: [6][24/38]\t Loss 0.2521 (0.3072)\t Accuracy 0.889 (0.881)\n",
            "Epoch: [6][25/38]\t Loss 0.1806 (0.3022)\t Accuracy 0.906 (0.882)\n",
            "Epoch: [6][26/38]\t Loss 0.3425 (0.3037)\t Accuracy 0.875 (0.882)\n",
            "Epoch: [6][27/38]\t Loss 0.2721 (0.3025)\t Accuracy 0.889 (0.882)\n",
            "Epoch: [6][28/38]\t Loss 0.4206 (0.3067)\t Accuracy 0.828 (0.880)\n",
            "Epoch: [6][29/38]\t Loss 0.2243 (0.3039)\t Accuracy 0.922 (0.882)\n",
            "Epoch: [6][30/38]\t Loss 0.2059 (0.3009)\t Accuracy 0.950 (0.884)\n",
            "Epoch: [6][31/38]\t Loss 0.3644 (0.3027)\t Accuracy 0.898 (0.884)\n",
            "Epoch: [6][32/38]\t Loss 0.2865 (0.3023)\t Accuracy 0.903 (0.885)\n",
            "Epoch: [6][33/38]\t Loss 0.2415 (0.3004)\t Accuracy 0.906 (0.885)\n",
            "Epoch: [6][34/38]\t Loss 0.2962 (0.3003)\t Accuracy 0.935 (0.887)\n",
            "Epoch: [6][35/38]\t Loss 0.2760 (0.2996)\t Accuracy 0.905 (0.887)\n",
            "Epoch: [6][36/38]\t Loss 0.5012 (0.3051)\t Accuracy 0.810 (0.885)\n",
            "Epoch: [6][37/38]\t Loss 0.8314 (0.3063)\t Accuracy 0.800 (0.885)\n",
            "Epoch: [6]\t Avg Loss 0.3063\t Avg Accuracy 0.885\n",
            "Test Average Accuracy: 0.7807\n",
            "Epoch: [7][0/38]\t Loss 0.2864 (0.2864)\t Accuracy 0.844 (0.844)\n",
            "Epoch: [7][1/38]\t Loss 0.2301 (0.2589)\t Accuracy 0.918 (0.880)\n",
            "Epoch: [7][2/38]\t Loss 0.2851 (0.2676)\t Accuracy 0.887 (0.882)\n",
            "Epoch: [7][3/38]\t Loss 0.2625 (0.2663)\t Accuracy 0.922 (0.892)\n",
            "Epoch: [7][4/38]\t Loss 0.2119 (0.2555)\t Accuracy 0.935 (0.901)\n",
            "Epoch: [7][5/38]\t Loss 0.3184 (0.2661)\t Accuracy 0.889 (0.899)\n",
            "Epoch: [7][6/38]\t Loss 0.3766 (0.2813)\t Accuracy 0.867 (0.894)\n",
            "Epoch: [7][7/38]\t Loss 0.2584 (0.2784)\t Accuracy 0.889 (0.894)\n",
            "Epoch: [7][8/38]\t Loss 0.2128 (0.2709)\t Accuracy 0.922 (0.897)\n",
            "Epoch: [7][9/38]\t Loss 0.1793 (0.2620)\t Accuracy 0.918 (0.899)\n",
            "Epoch: [7][10/38]\t Loss 0.1754 (0.2539)\t Accuracy 0.922 (0.901)\n",
            "Epoch: [7][11/38]\t Loss 0.2110 (0.2504)\t Accuracy 0.903 (0.901)\n",
            "Epoch: [7][12/38]\t Loss 0.1358 (0.2414)\t Accuracy 0.953 (0.905)\n",
            "Epoch: [7][13/38]\t Loss 0.2736 (0.2436)\t Accuracy 0.917 (0.906)\n",
            "Epoch: [7][14/38]\t Loss 0.2969 (0.2472)\t Accuracy 0.857 (0.903)\n",
            "Epoch: [7][15/38]\t Loss 0.2936 (0.2499)\t Accuracy 0.898 (0.903)\n",
            "Epoch: [7][16/38]\t Loss 0.3004 (0.2529)\t Accuracy 0.905 (0.903)\n",
            "Epoch: [7][17/38]\t Loss 0.4165 (0.2621)\t Accuracy 0.841 (0.899)\n",
            "Epoch: [7][18/38]\t Loss 0.1884 (0.2583)\t Accuracy 0.918 (0.900)\n",
            "Epoch: [7][19/38]\t Loss 0.2568 (0.2582)\t Accuracy 0.919 (0.901)\n",
            "Epoch: [7][20/38]\t Loss 0.1543 (0.2534)\t Accuracy 0.918 (0.902)\n",
            "Epoch: [7][21/38]\t Loss 0.5942 (0.2688)\t Accuracy 0.839 (0.899)\n",
            "Epoch: [7][22/38]\t Loss 0.3371 (0.2718)\t Accuracy 0.873 (0.898)\n",
            "Epoch: [7][23/38]\t Loss 0.2225 (0.2698)\t Accuracy 0.885 (0.897)\n",
            "Epoch: [7][24/38]\t Loss 0.2162 (0.2676)\t Accuracy 0.922 (0.898)\n",
            "Epoch: [7][25/38]\t Loss 0.3936 (0.2724)\t Accuracy 0.855 (0.897)\n",
            "Epoch: [7][26/38]\t Loss 0.2976 (0.2733)\t Accuracy 0.885 (0.896)\n",
            "Epoch: [7][27/38]\t Loss 0.3306 (0.2754)\t Accuracy 0.889 (0.896)\n",
            "Epoch: [7][28/38]\t Loss 0.3407 (0.2777)\t Accuracy 0.887 (0.896)\n",
            "Epoch: [7][29/38]\t Loss 0.2012 (0.2751)\t Accuracy 0.905 (0.896)\n",
            "Epoch: [7][30/38]\t Loss 0.2849 (0.2754)\t Accuracy 0.875 (0.895)\n",
            "Epoch: [7][31/38]\t Loss 0.2111 (0.2734)\t Accuracy 0.919 (0.896)\n",
            "Epoch: [7][32/38]\t Loss 0.3736 (0.2765)\t Accuracy 0.810 (0.893)\n",
            "Epoch: [7][33/38]\t Loss 0.3748 (0.2794)\t Accuracy 0.855 (0.892)\n",
            "Epoch: [7][34/38]\t Loss 0.2868 (0.2796)\t Accuracy 0.873 (0.892)\n",
            "Epoch: [7][35/38]\t Loss 0.1420 (0.2756)\t Accuracy 0.953 (0.894)\n",
            "Epoch: [7][36/38]\t Loss 0.2630 (0.2753)\t Accuracy 0.887 (0.893)\n",
            "Epoch: [7][37/38]\t Loss 0.3703 (0.2755)\t Accuracy 0.800 (0.893)\n",
            "Epoch: [7]\t Avg Loss 0.2755\t Avg Accuracy 0.893\n",
            "Test Average Accuracy: 0.7787\n",
            "Epoch: [8][0/38]\t Loss 0.1448 (0.1448)\t Accuracy 0.967 (0.967)\n",
            "Epoch: [8][1/38]\t Loss 0.2571 (0.2000)\t Accuracy 0.881 (0.925)\n",
            "Epoch: [8][2/38]\t Loss 0.1776 (0.1923)\t Accuracy 0.905 (0.918)\n",
            "Epoch: [8][3/38]\t Loss 0.1874 (0.1911)\t Accuracy 0.935 (0.922)\n",
            "Epoch: [8][4/38]\t Loss 0.1409 (0.1811)\t Accuracy 0.934 (0.925)\n",
            "Epoch: [8][5/38]\t Loss 0.2391 (0.1910)\t Accuracy 0.921 (0.924)\n",
            "Epoch: [8][6/38]\t Loss 0.1761 (0.1888)\t Accuracy 0.921 (0.924)\n",
            "Epoch: [8][7/38]\t Loss 0.1591 (0.1851)\t Accuracy 0.952 (0.927)\n",
            "Epoch: [8][8/38]\t Loss 0.1310 (0.1790)\t Accuracy 0.935 (0.928)\n",
            "Epoch: [8][9/38]\t Loss 0.2845 (0.1896)\t Accuracy 0.903 (0.926)\n",
            "Epoch: [8][10/38]\t Loss 0.1589 (0.1868)\t Accuracy 0.968 (0.930)\n",
            "Epoch: [8][11/38]\t Loss 0.1782 (0.1861)\t Accuracy 0.937 (0.930)\n",
            "Epoch: [8][12/38]\t Loss 0.2032 (0.1874)\t Accuracy 0.905 (0.928)\n",
            "Epoch: [8][13/38]\t Loss 0.1503 (0.1847)\t Accuracy 0.938 (0.929)\n",
            "Epoch: [8][14/38]\t Loss 0.2667 (0.1903)\t Accuracy 0.922 (0.928)\n",
            "Epoch: [8][15/38]\t Loss 0.3394 (0.1996)\t Accuracy 0.871 (0.925)\n",
            "Epoch: [8][16/38]\t Loss 0.3051 (0.2055)\t Accuracy 0.883 (0.922)\n",
            "Epoch: [8][17/38]\t Loss 0.2497 (0.2080)\t Accuracy 0.905 (0.921)\n",
            "Epoch: [8][18/38]\t Loss 0.3701 (0.2168)\t Accuracy 0.859 (0.918)\n",
            "Epoch: [8][19/38]\t Loss 0.2717 (0.2196)\t Accuracy 0.938 (0.919)\n",
            "Epoch: [8][20/38]\t Loss 0.3431 (0.2255)\t Accuracy 0.905 (0.918)\n",
            "Epoch: [8][21/38]\t Loss 0.2868 (0.2284)\t Accuracy 0.889 (0.917)\n",
            "Epoch: [8][22/38]\t Loss 0.2075 (0.2275)\t Accuracy 0.935 (0.918)\n",
            "Epoch: [8][23/38]\t Loss 0.1853 (0.2257)\t Accuracy 0.952 (0.919)\n",
            "Epoch: [8][24/38]\t Loss 0.2038 (0.2248)\t Accuracy 0.922 (0.919)\n",
            "Epoch: [8][25/38]\t Loss 0.0953 (0.2198)\t Accuracy 0.952 (0.921)\n",
            "Epoch: [8][26/38]\t Loss 0.2501 (0.2209)\t Accuracy 0.902 (0.920)\n",
            "Epoch: [8][27/38]\t Loss 0.3823 (0.2267)\t Accuracy 0.889 (0.919)\n",
            "Epoch: [8][28/38]\t Loss 0.2427 (0.2272)\t Accuracy 0.933 (0.919)\n",
            "Epoch: [8][29/38]\t Loss 0.2216 (0.2270)\t Accuracy 0.935 (0.920)\n",
            "Epoch: [8][30/38]\t Loss 0.2529 (0.2279)\t Accuracy 0.887 (0.919)\n",
            "Epoch: [8][31/38]\t Loss 0.2798 (0.2294)\t Accuracy 0.885 (0.918)\n",
            "Epoch: [8][32/38]\t Loss 0.2518 (0.2301)\t Accuracy 0.889 (0.917)\n",
            "Epoch: [8][33/38]\t Loss 0.1562 (0.2280)\t Accuracy 0.951 (0.918)\n",
            "Epoch: [8][34/38]\t Loss 0.2775 (0.2295)\t Accuracy 0.906 (0.918)\n",
            "Epoch: [8][35/38]\t Loss 0.1406 (0.2270)\t Accuracy 0.934 (0.918)\n",
            "Epoch: [8][36/38]\t Loss 0.2829 (0.2286)\t Accuracy 0.921 (0.918)\n",
            "Epoch: [8][37/38]\t Loss 0.3323 (0.2288)\t Accuracy 1.000 (0.918)\n",
            "Epoch: [8]\t Avg Loss 0.2288\t Avg Accuracy 0.918\n",
            "Test Average Accuracy: 0.7715\n",
            "Epoch: [9][0/38]\t Loss 0.2823 (0.2823)\t Accuracy 0.873 (0.873)\n",
            "Epoch: [9][1/38]\t Loss 0.2081 (0.2455)\t Accuracy 0.919 (0.896)\n",
            "Epoch: [9][2/38]\t Loss 0.1787 (0.2231)\t Accuracy 0.921 (0.904)\n",
            "Epoch: [9][3/38]\t Loss 0.1546 (0.2059)\t Accuracy 0.952 (0.916)\n",
            "Epoch: [9][4/38]\t Loss 0.2045 (0.2056)\t Accuracy 0.937 (0.920)\n",
            "Epoch: [9][5/38]\t Loss 0.2960 (0.2207)\t Accuracy 0.889 (0.915)\n",
            "Epoch: [9][6/38]\t Loss 0.1134 (0.2058)\t Accuracy 0.951 (0.920)\n",
            "Epoch: [9][7/38]\t Loss 0.1081 (0.1935)\t Accuracy 0.984 (0.928)\n",
            "Epoch: [9][8/38]\t Loss 0.1677 (0.1907)\t Accuracy 0.952 (0.931)\n",
            "Epoch: [9][9/38]\t Loss 0.2267 (0.1941)\t Accuracy 0.933 (0.931)\n",
            "Epoch: [9][10/38]\t Loss 0.1497 (0.1900)\t Accuracy 0.938 (0.932)\n",
            "Epoch: [9][11/38]\t Loss 0.3935 (0.2071)\t Accuracy 0.873 (0.927)\n",
            "Epoch: [9][12/38]\t Loss 0.1432 (0.2021)\t Accuracy 0.952 (0.929)\n",
            "Epoch: [9][13/38]\t Loss 0.1803 (0.2006)\t Accuracy 0.937 (0.929)\n",
            "Epoch: [9][14/38]\t Loss 0.2225 (0.2020)\t Accuracy 0.889 (0.927)\n",
            "Epoch: [9][15/38]\t Loss 0.2051 (0.2022)\t Accuracy 0.922 (0.926)\n",
            "Epoch: [9][16/38]\t Loss 0.3534 (0.2109)\t Accuracy 0.902 (0.925)\n",
            "Epoch: [9][17/38]\t Loss 0.3384 (0.2180)\t Accuracy 0.905 (0.924)\n",
            "Epoch: [9][18/38]\t Loss 0.1307 (0.2136)\t Accuracy 0.917 (0.923)\n",
            "Epoch: [9][19/38]\t Loss 0.1627 (0.2111)\t Accuracy 0.952 (0.925)\n",
            "Epoch: [9][20/38]\t Loss 0.2521 (0.2130)\t Accuracy 0.918 (0.924)\n",
            "Epoch: [9][21/38]\t Loss 0.1311 (0.2092)\t Accuracy 0.937 (0.925)\n",
            "Epoch: [9][22/38]\t Loss 0.1265 (0.2057)\t Accuracy 0.952 (0.926)\n",
            "Epoch: [9][23/38]\t Loss 0.2069 (0.2057)\t Accuracy 0.952 (0.927)\n",
            "Epoch: [9][24/38]\t Loss 0.2269 (0.2065)\t Accuracy 0.879 (0.925)\n",
            "Epoch: [9][25/38]\t Loss 0.3249 (0.2111)\t Accuracy 0.905 (0.925)\n",
            "Epoch: [9][26/38]\t Loss 0.2711 (0.2133)\t Accuracy 0.903 (0.924)\n",
            "Epoch: [9][27/38]\t Loss 0.1936 (0.2126)\t Accuracy 0.937 (0.924)\n",
            "Epoch: [9][28/38]\t Loss 0.2365 (0.2134)\t Accuracy 0.919 (0.924)\n",
            "Epoch: [9][29/38]\t Loss 0.0687 (0.2086)\t Accuracy 0.984 (0.926)\n",
            "Epoch: [9][30/38]\t Loss 0.1973 (0.2082)\t Accuracy 0.919 (0.926)\n",
            "Epoch: [9][31/38]\t Loss 0.2032 (0.2080)\t Accuracy 0.902 (0.925)\n",
            "Epoch: [9][32/38]\t Loss 0.2397 (0.2090)\t Accuracy 0.875 (0.924)\n",
            "Epoch: [9][33/38]\t Loss 0.1590 (0.2075)\t Accuracy 0.921 (0.924)\n",
            "Epoch: [9][34/38]\t Loss 0.2315 (0.2082)\t Accuracy 0.921 (0.923)\n",
            "Epoch: [9][35/38]\t Loss 0.1159 (0.2057)\t Accuracy 0.952 (0.924)\n",
            "Epoch: [9][36/38]\t Loss 0.2971 (0.2082)\t Accuracy 0.873 (0.923)\n",
            "Epoch: [9][37/38]\t Loss 0.3381 (0.2085)\t Accuracy 0.800 (0.923)\n",
            "Epoch: [9]\t Avg Loss 0.2085\t Avg Accuracy 0.923\n",
            "Test Average Accuracy: 0.7695\n",
            "Epoch: [10][0/38]\t Loss 0.2151 (0.2151)\t Accuracy 0.921 (0.921)\n",
            "Epoch: [10][1/38]\t Loss 0.1812 (0.1983)\t Accuracy 0.935 (0.928)\n",
            "Epoch: [10][2/38]\t Loss 0.1840 (0.1935)\t Accuracy 0.937 (0.931)\n",
            "Epoch: [10][3/38]\t Loss 0.1049 (0.1715)\t Accuracy 0.968 (0.940)\n",
            "Epoch: [10][4/38]\t Loss 0.2153 (0.1803)\t Accuracy 0.889 (0.930)\n",
            "Epoch: [10][5/38]\t Loss 0.2179 (0.1866)\t Accuracy 0.937 (0.931)\n",
            "Epoch: [10][6/38]\t Loss 0.2130 (0.1904)\t Accuracy 0.921 (0.929)\n",
            "Epoch: [10][7/38]\t Loss 0.1215 (0.1820)\t Accuracy 0.967 (0.934)\n",
            "Epoch: [10][8/38]\t Loss 0.1819 (0.1820)\t Accuracy 0.932 (0.934)\n",
            "Epoch: [10][9/38]\t Loss 0.1587 (0.1796)\t Accuracy 0.937 (0.934)\n",
            "Epoch: [10][10/38]\t Loss 0.1246 (0.1747)\t Accuracy 0.968 (0.937)\n",
            "Epoch: [10][11/38]\t Loss 0.1399 (0.1717)\t Accuracy 0.952 (0.938)\n",
            "Epoch: [10][12/38]\t Loss 0.2718 (0.1793)\t Accuracy 0.934 (0.938)\n",
            "Epoch: [10][13/38]\t Loss 0.2536 (0.1844)\t Accuracy 0.900 (0.935)\n",
            "Epoch: [10][14/38]\t Loss 0.1374 (0.1812)\t Accuracy 0.953 (0.937)\n",
            "Epoch: [10][15/38]\t Loss 0.1881 (0.1816)\t Accuracy 0.935 (0.937)\n",
            "Epoch: [10][16/38]\t Loss 0.2882 (0.1879)\t Accuracy 0.871 (0.933)\n",
            "Epoch: [10][17/38]\t Loss 0.1409 (0.1853)\t Accuracy 0.952 (0.934)\n",
            "Epoch: [10][18/38]\t Loss 0.2341 (0.1878)\t Accuracy 0.887 (0.931)\n",
            "Epoch: [10][19/38]\t Loss 0.1948 (0.1882)\t Accuracy 0.921 (0.931)\n",
            "Epoch: [10][20/38]\t Loss 0.2087 (0.1892)\t Accuracy 0.921 (0.930)\n",
            "Epoch: [10][21/38]\t Loss 0.0959 (0.1850)\t Accuracy 0.968 (0.932)\n",
            "Epoch: [10][22/38]\t Loss 0.1880 (0.1851)\t Accuracy 0.917 (0.931)\n",
            "Epoch: [10][23/38]\t Loss 0.1724 (0.1845)\t Accuracy 0.903 (0.930)\n",
            "Epoch: [10][24/38]\t Loss 0.1550 (0.1833)\t Accuracy 0.938 (0.931)\n",
            "Epoch: [10][25/38]\t Loss 0.1036 (0.1803)\t Accuracy 0.952 (0.931)\n",
            "Epoch: [10][26/38]\t Loss 0.1373 (0.1786)\t Accuracy 0.938 (0.932)\n",
            "Epoch: [10][27/38]\t Loss 0.2064 (0.1796)\t Accuracy 0.933 (0.932)\n",
            "Epoch: [10][28/38]\t Loss 0.3526 (0.1856)\t Accuracy 0.889 (0.930)\n",
            "Epoch: [10][29/38]\t Loss 0.1597 (0.1848)\t Accuracy 0.935 (0.930)\n",
            "Epoch: [10][30/38]\t Loss 0.2806 (0.1879)\t Accuracy 0.905 (0.929)\n",
            "Epoch: [10][31/38]\t Loss 0.0860 (0.1847)\t Accuracy 0.984 (0.931)\n",
            "Epoch: [10][32/38]\t Loss 0.2082 (0.1854)\t Accuracy 0.906 (0.930)\n",
            "Epoch: [10][33/38]\t Loss 0.3069 (0.1891)\t Accuracy 0.875 (0.929)\n",
            "Epoch: [10][34/38]\t Loss 0.2569 (0.1910)\t Accuracy 0.902 (0.928)\n",
            "Epoch: [10][35/38]\t Loss 0.1718 (0.1904)\t Accuracy 0.937 (0.928)\n",
            "Epoch: [10][36/38]\t Loss 0.0880 (0.1876)\t Accuracy 0.984 (0.930)\n",
            "Epoch: [10][37/38]\t Loss 0.0044 (0.1872)\t Accuracy 1.000 (0.930)\n",
            "Epoch: [10]\t Avg Loss 0.1872\t Avg Accuracy 0.930\n",
            "Test Average Accuracy: 0.7787\n",
            "Epoch: [11][0/38]\t Loss 0.1263 (0.1263)\t Accuracy 0.969 (0.969)\n",
            "Epoch: [11][1/38]\t Loss 0.1492 (0.1377)\t Accuracy 0.937 (0.953)\n",
            "Epoch: [11][2/38]\t Loss 0.1918 (0.1558)\t Accuracy 0.906 (0.937)\n",
            "Epoch: [11][3/38]\t Loss 0.1444 (0.1530)\t Accuracy 0.968 (0.945)\n",
            "Epoch: [11][4/38]\t Loss 0.0925 (0.1411)\t Accuracy 0.968 (0.949)\n",
            "Epoch: [11][5/38]\t Loss 0.1878 (0.1490)\t Accuracy 0.938 (0.947)\n",
            "Epoch: [11][6/38]\t Loss 0.1261 (0.1458)\t Accuracy 0.952 (0.948)\n",
            "Epoch: [11][7/38]\t Loss 0.0793 (0.1376)\t Accuracy 0.984 (0.952)\n",
            "Epoch: [11][8/38]\t Loss 0.1414 (0.1380)\t Accuracy 0.952 (0.952)\n",
            "Epoch: [11][9/38]\t Loss 0.1596 (0.1402)\t Accuracy 0.919 (0.949)\n",
            "Epoch: [11][10/38]\t Loss 0.1731 (0.1430)\t Accuracy 0.933 (0.948)\n",
            "Epoch: [11][11/38]\t Loss 0.1555 (0.1440)\t Accuracy 0.933 (0.947)\n",
            "Epoch: [11][12/38]\t Loss 0.2023 (0.1485)\t Accuracy 0.889 (0.942)\n",
            "Epoch: [11][13/38]\t Loss 0.1027 (0.1453)\t Accuracy 0.952 (0.943)\n",
            "Epoch: [11][14/38]\t Loss 0.2617 (0.1531)\t Accuracy 0.889 (0.939)\n",
            "Epoch: [11][15/38]\t Loss 0.2414 (0.1587)\t Accuracy 0.921 (0.938)\n",
            "Epoch: [11][16/38]\t Loss 0.0886 (0.1545)\t Accuracy 0.968 (0.940)\n",
            "Epoch: [11][17/38]\t Loss 0.1093 (0.1520)\t Accuracy 0.968 (0.941)\n",
            "Epoch: [11][18/38]\t Loss 0.2713 (0.1584)\t Accuracy 0.891 (0.939)\n",
            "Epoch: [11][19/38]\t Loss 0.0890 (0.1549)\t Accuracy 0.968 (0.940)\n",
            "Epoch: [11][20/38]\t Loss 0.1416 (0.1543)\t Accuracy 0.918 (0.939)\n",
            "Epoch: [11][21/38]\t Loss 0.2401 (0.1580)\t Accuracy 0.915 (0.938)\n",
            "Epoch: [11][22/38]\t Loss 0.1594 (0.1581)\t Accuracy 0.952 (0.939)\n",
            "Epoch: [11][23/38]\t Loss 0.1330 (0.1570)\t Accuracy 0.935 (0.939)\n",
            "Epoch: [11][24/38]\t Loss 0.1377 (0.1562)\t Accuracy 0.937 (0.938)\n",
            "Epoch: [11][25/38]\t Loss 0.1766 (0.1570)\t Accuracy 0.903 (0.937)\n",
            "Epoch: [11][26/38]\t Loss 0.1863 (0.1581)\t Accuracy 0.921 (0.936)\n",
            "Epoch: [11][27/38]\t Loss 0.0852 (0.1555)\t Accuracy 0.968 (0.938)\n",
            "Epoch: [11][28/38]\t Loss 0.0807 (0.1530)\t Accuracy 0.983 (0.939)\n",
            "Epoch: [11][29/38]\t Loss 0.1805 (0.1540)\t Accuracy 0.906 (0.938)\n",
            "Epoch: [11][30/38]\t Loss 0.1123 (0.1526)\t Accuracy 0.969 (0.939)\n",
            "Epoch: [11][31/38]\t Loss 0.1538 (0.1526)\t Accuracy 0.921 (0.938)\n",
            "Epoch: [11][32/38]\t Loss 0.0846 (0.1506)\t Accuracy 0.952 (0.939)\n",
            "Epoch: [11][33/38]\t Loss 0.1051 (0.1493)\t Accuracy 0.984 (0.940)\n",
            "Epoch: [11][34/38]\t Loss 0.1168 (0.1483)\t Accuracy 0.952 (0.940)\n",
            "Epoch: [11][35/38]\t Loss 0.1469 (0.1483)\t Accuracy 0.952 (0.941)\n",
            "Epoch: [11][36/38]\t Loss 0.0497 (0.1457)\t Accuracy 1.000 (0.942)\n",
            "Epoch: [11][37/38]\t Loss 0.0073 (0.1454)\t Accuracy 1.000 (0.942)\n",
            "Epoch: [11]\t Avg Loss 0.1454\t Avg Accuracy 0.942\n",
            "Test Average Accuracy: 0.7866\n",
            "Saving Model...\n",
            "Epoch: [12][0/38]\t Loss 0.3490 (0.3490)\t Accuracy 0.857 (0.857)\n",
            "Epoch: [12][1/38]\t Loss 0.0294 (0.1879)\t Accuracy 1.000 (0.929)\n",
            "Epoch: [12][2/38]\t Loss 0.1919 (0.1893)\t Accuracy 0.922 (0.927)\n",
            "Epoch: [12][3/38]\t Loss 0.0360 (0.1513)\t Accuracy 1.000 (0.945)\n",
            "Epoch: [12][4/38]\t Loss 0.1004 (0.1412)\t Accuracy 0.952 (0.946)\n",
            "Epoch: [12][5/38]\t Loss 0.0572 (0.1276)\t Accuracy 0.984 (0.952)\n",
            "Epoch: [12][6/38]\t Loss 0.0657 (0.1188)\t Accuracy 0.968 (0.955)\n",
            "Epoch: [12][7/38]\t Loss 0.0480 (0.1102)\t Accuracy 1.000 (0.960)\n",
            "Epoch: [12][8/38]\t Loss 0.0877 (0.1076)\t Accuracy 0.984 (0.963)\n",
            "Epoch: [12][9/38]\t Loss 0.1095 (0.1078)\t Accuracy 0.968 (0.963)\n",
            "Epoch: [12][10/38]\t Loss 0.0883 (0.1061)\t Accuracy 0.967 (0.964)\n",
            "Epoch: [12][11/38]\t Loss 0.1334 (0.1084)\t Accuracy 0.937 (0.961)\n",
            "Epoch: [12][12/38]\t Loss 0.1289 (0.1100)\t Accuracy 0.969 (0.962)\n",
            "Epoch: [12][13/38]\t Loss 0.1178 (0.1105)\t Accuracy 0.949 (0.961)\n",
            "Epoch: [12][14/38]\t Loss 0.0938 (0.1094)\t Accuracy 0.984 (0.963)\n",
            "Epoch: [12][15/38]\t Loss 0.0874 (0.1081)\t Accuracy 0.968 (0.963)\n",
            "Epoch: [12][16/38]\t Loss 0.1658 (0.1115)\t Accuracy 0.953 (0.962)\n",
            "Epoch: [12][17/38]\t Loss 0.0353 (0.1073)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [12][18/38]\t Loss 0.0296 (0.1032)\t Accuracy 1.000 (0.966)\n",
            "Epoch: [12][19/38]\t Loss 0.0848 (0.1022)\t Accuracy 0.968 (0.966)\n",
            "Epoch: [12][20/38]\t Loss 0.2807 (0.1108)\t Accuracy 0.889 (0.962)\n",
            "Epoch: [12][21/38]\t Loss 0.1000 (0.1103)\t Accuracy 0.933 (0.961)\n",
            "Epoch: [12][22/38]\t Loss 0.1072 (0.1102)\t Accuracy 0.953 (0.960)\n",
            "Epoch: [12][23/38]\t Loss 0.1899 (0.1135)\t Accuracy 0.952 (0.960)\n",
            "Epoch: [12][24/38]\t Loss 0.1452 (0.1147)\t Accuracy 0.967 (0.960)\n",
            "Epoch: [12][25/38]\t Loss 0.0546 (0.1124)\t Accuracy 0.984 (0.961)\n",
            "Epoch: [12][26/38]\t Loss 0.0781 (0.1111)\t Accuracy 0.984 (0.962)\n",
            "Epoch: [12][27/38]\t Loss 0.3165 (0.1184)\t Accuracy 0.919 (0.961)\n",
            "Epoch: [12][28/38]\t Loss 0.2428 (0.1227)\t Accuracy 0.919 (0.959)\n",
            "Epoch: [12][29/38]\t Loss 0.0992 (0.1219)\t Accuracy 0.952 (0.959)\n",
            "Epoch: [12][30/38]\t Loss 0.0870 (0.1208)\t Accuracy 0.984 (0.960)\n",
            "Epoch: [12][31/38]\t Loss 0.1748 (0.1225)\t Accuracy 0.937 (0.959)\n",
            "Epoch: [12][32/38]\t Loss 0.1742 (0.1240)\t Accuracy 0.967 (0.959)\n",
            "Epoch: [12][33/38]\t Loss 0.1001 (0.1233)\t Accuracy 0.952 (0.959)\n",
            "Epoch: [12][34/38]\t Loss 0.1893 (0.1252)\t Accuracy 0.889 (0.957)\n",
            "Epoch: [12][35/38]\t Loss 0.0799 (0.1240)\t Accuracy 0.967 (0.957)\n",
            "Epoch: [12][36/38]\t Loss 0.1464 (0.1246)\t Accuracy 0.935 (0.957)\n",
            "Epoch: [12][37/38]\t Loss 0.0403 (0.1244)\t Accuracy 1.000 (0.957)\n",
            "Epoch: [12]\t Avg Loss 0.1244\t Avg Accuracy 0.957\n",
            "Test Average Accuracy: 0.7853\n",
            "Epoch: [13][0/38]\t Loss 0.1203 (0.1203)\t Accuracy 0.968 (0.968)\n",
            "Epoch: [13][1/38]\t Loss 0.0704 (0.0952)\t Accuracy 0.969 (0.969)\n",
            "Epoch: [13][2/38]\t Loss 0.0842 (0.0915)\t Accuracy 0.968 (0.968)\n",
            "Epoch: [13][3/38]\t Loss 0.0423 (0.0793)\t Accuracy 0.984 (0.972)\n",
            "Epoch: [13][4/38]\t Loss 0.2944 (0.1227)\t Accuracy 0.891 (0.956)\n",
            "Epoch: [13][5/38]\t Loss 0.1072 (0.1202)\t Accuracy 0.934 (0.952)\n",
            "Epoch: [13][6/38]\t Loss 0.0339 (0.1082)\t Accuracy 1.000 (0.959)\n",
            "Epoch: [13][7/38]\t Loss 0.0137 (0.0964)\t Accuracy 1.000 (0.964)\n",
            "Epoch: [13][8/38]\t Loss 0.0716 (0.0936)\t Accuracy 0.984 (0.966)\n",
            "Epoch: [13][9/38]\t Loss 0.2955 (0.1136)\t Accuracy 0.935 (0.963)\n",
            "Epoch: [13][10/38]\t Loss 0.3322 (0.1333)\t Accuracy 0.919 (0.959)\n",
            "Epoch: [13][11/38]\t Loss 0.0738 (0.1284)\t Accuracy 0.984 (0.961)\n",
            "Epoch: [13][12/38]\t Loss 0.1315 (0.1286)\t Accuracy 0.938 (0.959)\n",
            "Epoch: [13][13/38]\t Loss 0.0689 (0.1243)\t Accuracy 0.969 (0.960)\n",
            "Epoch: [13][14/38]\t Loss 0.1159 (0.1237)\t Accuracy 0.953 (0.960)\n",
            "Epoch: [13][15/38]\t Loss 0.0706 (0.1205)\t Accuracy 0.984 (0.961)\n",
            "Epoch: [13][16/38]\t Loss 0.0748 (0.1178)\t Accuracy 0.984 (0.963)\n",
            "Epoch: [13][17/38]\t Loss 0.1260 (0.1182)\t Accuracy 0.968 (0.963)\n",
            "Epoch: [13][18/38]\t Loss 0.1147 (0.1180)\t Accuracy 0.969 (0.963)\n",
            "Epoch: [13][19/38]\t Loss 0.1381 (0.1190)\t Accuracy 0.934 (0.962)\n",
            "Epoch: [13][20/38]\t Loss 0.0846 (0.1174)\t Accuracy 0.984 (0.963)\n",
            "Epoch: [13][21/38]\t Loss 0.1091 (0.1170)\t Accuracy 0.968 (0.963)\n",
            "Epoch: [13][22/38]\t Loss 0.0464 (0.1139)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [13][23/38]\t Loss 0.0213 (0.1102)\t Accuracy 1.000 (0.965)\n",
            "Epoch: [13][24/38]\t Loss 0.1619 (0.1122)\t Accuracy 0.950 (0.965)\n",
            "Epoch: [13][25/38]\t Loss 0.0806 (0.1110)\t Accuracy 0.967 (0.965)\n",
            "Epoch: [13][26/38]\t Loss 0.0556 (0.1090)\t Accuracy 0.968 (0.965)\n",
            "Epoch: [13][27/38]\t Loss 0.2004 (0.1122)\t Accuracy 0.935 (0.964)\n",
            "Epoch: [13][28/38]\t Loss 0.1544 (0.1137)\t Accuracy 0.935 (0.963)\n",
            "Epoch: [13][29/38]\t Loss 0.0644 (0.1120)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [13][30/38]\t Loss 0.1450 (0.1130)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [13][31/38]\t Loss 0.1024 (0.1127)\t Accuracy 0.951 (0.964)\n",
            "Epoch: [13][32/38]\t Loss 0.0494 (0.1108)\t Accuracy 0.984 (0.965)\n",
            "Epoch: [13][33/38]\t Loss 0.0947 (0.1103)\t Accuracy 0.951 (0.964)\n",
            "Epoch: [13][34/38]\t Loss 0.0963 (0.1099)\t Accuracy 0.952 (0.964)\n",
            "Epoch: [13][35/38]\t Loss 0.1787 (0.1118)\t Accuracy 0.968 (0.964)\n",
            "Epoch: [13][36/38]\t Loss 0.0918 (0.1113)\t Accuracy 0.968 (0.964)\n",
            "Epoch: [13][37/38]\t Loss 0.0759 (0.1112)\t Accuracy 1.000 (0.964)\n",
            "Epoch: [13]\t Avg Loss 0.1112\t Avg Accuracy 0.964\n",
            "Test Average Accuracy: 0.7800\n",
            "Epoch: [14][0/38]\t Loss 0.0585 (0.0585)\t Accuracy 0.969 (0.969)\n",
            "Epoch: [14][1/38]\t Loss 0.0504 (0.0545)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [14][2/38]\t Loss 0.1279 (0.0778)\t Accuracy 0.949 (0.968)\n",
            "Epoch: [14][3/38]\t Loss 0.1118 (0.0863)\t Accuracy 0.952 (0.964)\n",
            "Epoch: [14][4/38]\t Loss 0.1305 (0.0951)\t Accuracy 0.952 (0.961)\n",
            "Epoch: [14][5/38]\t Loss 0.1613 (0.1063)\t Accuracy 0.937 (0.957)\n",
            "Epoch: [14][6/38]\t Loss 0.0993 (0.1053)\t Accuracy 0.984 (0.961)\n",
            "Epoch: [14][7/38]\t Loss 0.0636 (0.1000)\t Accuracy 0.968 (0.962)\n",
            "Epoch: [14][8/38]\t Loss 0.0219 (0.0913)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [14][9/38]\t Loss 0.0847 (0.0906)\t Accuracy 0.968 (0.965)\n",
            "Epoch: [14][10/38]\t Loss 0.1526 (0.0964)\t Accuracy 0.953 (0.964)\n",
            "Epoch: [14][11/38]\t Loss 0.1617 (0.1018)\t Accuracy 0.935 (0.961)\n",
            "Epoch: [14][12/38]\t Loss 0.1097 (0.1024)\t Accuracy 0.952 (0.961)\n",
            "Epoch: [14][13/38]\t Loss 0.0305 (0.0972)\t Accuracy 0.984 (0.962)\n",
            "Epoch: [14][14/38]\t Loss 0.0830 (0.0963)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [14][15/38]\t Loss 0.0534 (0.0936)\t Accuracy 0.984 (0.965)\n",
            "Epoch: [14][16/38]\t Loss 0.0652 (0.0919)\t Accuracy 0.984 (0.966)\n",
            "Epoch: [14][17/38]\t Loss 0.0548 (0.0900)\t Accuracy 0.983 (0.967)\n",
            "Epoch: [14][18/38]\t Loss 0.1434 (0.0927)\t Accuracy 0.934 (0.965)\n",
            "Epoch: [14][19/38]\t Loss 0.1677 (0.0965)\t Accuracy 0.922 (0.963)\n",
            "Epoch: [14][20/38]\t Loss 0.0608 (0.0949)\t Accuracy 0.984 (0.964)\n",
            "Epoch: [14][21/38]\t Loss 0.0441 (0.0926)\t Accuracy 1.000 (0.966)\n",
            "Epoch: [14][22/38]\t Loss 0.0858 (0.0923)\t Accuracy 0.967 (0.966)\n",
            "Epoch: [14][23/38]\t Loss 0.1233 (0.0937)\t Accuracy 0.937 (0.965)\n",
            "Epoch: [14][24/38]\t Loss 0.3446 (0.1036)\t Accuracy 0.919 (0.963)\n",
            "Epoch: [14][25/38]\t Loss 0.1119 (0.1040)\t Accuracy 0.968 (0.963)\n",
            "Epoch: [14][26/38]\t Loss 0.0294 (0.1012)\t Accuracy 1.000 (0.964)\n",
            "Epoch: [14][27/38]\t Loss 0.0894 (0.1008)\t Accuracy 0.951 (0.964)\n",
            "Epoch: [14][28/38]\t Loss 0.0726 (0.0998)\t Accuracy 0.984 (0.965)\n",
            "Epoch: [14][29/38]\t Loss 0.0484 (0.0981)\t Accuracy 0.968 (0.965)\n",
            "Epoch: [14][30/38]\t Loss 0.1357 (0.0993)\t Accuracy 0.969 (0.965)\n",
            "Epoch: [14][31/38]\t Loss 0.1947 (0.1023)\t Accuracy 0.952 (0.964)\n",
            "Epoch: [14][32/38]\t Loss 0.0441 (0.1006)\t Accuracy 0.984 (0.965)\n",
            "Epoch: [14][33/38]\t Loss 0.0540 (0.0992)\t Accuracy 0.968 (0.965)\n",
            "Epoch: [14][34/38]\t Loss 0.0389 (0.0974)\t Accuracy 0.984 (0.966)\n",
            "Epoch: [14][35/38]\t Loss 0.1099 (0.0978)\t Accuracy 0.968 (0.966)\n",
            "Epoch: [14][36/38]\t Loss 0.0919 (0.0976)\t Accuracy 0.952 (0.965)\n",
            "Epoch: [14][37/38]\t Loss 0.0149 (0.0974)\t Accuracy 1.000 (0.965)\n",
            "Epoch: [14]\t Avg Loss 0.0974\t Avg Accuracy 0.965\n",
            "Test Average Accuracy: 0.7708\n",
            "Epoch: [15][0/38]\t Loss 0.1486 (0.1486)\t Accuracy 0.952 (0.952)\n",
            "Epoch: [15][1/38]\t Loss 0.0437 (0.0966)\t Accuracy 0.984 (0.967)\n",
            "Epoch: [15][2/38]\t Loss 0.1771 (0.1239)\t Accuracy 0.937 (0.957)\n",
            "Epoch: [15][3/38]\t Loss 0.0741 (0.1113)\t Accuracy 0.968 (0.960)\n",
            "Epoch: [15][4/38]\t Loss 0.0490 (0.0989)\t Accuracy 0.984 (0.965)\n",
            "Epoch: [15][5/38]\t Loss 0.0295 (0.0872)\t Accuracy 1.000 (0.971)\n",
            "Epoch: [15][6/38]\t Loss 0.0702 (0.0847)\t Accuracy 0.984 (0.973)\n",
            "Epoch: [15][7/38]\t Loss 0.1050 (0.0873)\t Accuracy 0.952 (0.970)\n",
            "Epoch: [15][8/38]\t Loss 0.0456 (0.0827)\t Accuracy 0.968 (0.970)\n",
            "Epoch: [15][9/38]\t Loss 0.0563 (0.0800)\t Accuracy 0.984 (0.971)\n",
            "Epoch: [15][10/38]\t Loss 0.0209 (0.0747)\t Accuracy 1.000 (0.974)\n",
            "Epoch: [15][11/38]\t Loss 0.0271 (0.0707)\t Accuracy 1.000 (0.976)\n",
            "Epoch: [15][12/38]\t Loss 0.0086 (0.0660)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [15][13/38]\t Loss 0.2324 (0.0778)\t Accuracy 0.952 (0.976)\n",
            "Epoch: [15][14/38]\t Loss 0.0458 (0.0757)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [15][15/38]\t Loss 0.1715 (0.0818)\t Accuracy 0.953 (0.975)\n",
            "Epoch: [15][16/38]\t Loss 0.1420 (0.0853)\t Accuracy 0.951 (0.974)\n",
            "Epoch: [15][17/38]\t Loss 0.1508 (0.0889)\t Accuracy 0.919 (0.971)\n",
            "Epoch: [15][18/38]\t Loss 0.0946 (0.0892)\t Accuracy 0.953 (0.970)\n",
            "Epoch: [15][19/38]\t Loss 0.4360 (0.1059)\t Accuracy 0.900 (0.966)\n",
            "Epoch: [15][20/38]\t Loss 0.0962 (0.1055)\t Accuracy 0.933 (0.965)\n",
            "Epoch: [15][21/38]\t Loss 0.3653 (0.1176)\t Accuracy 0.906 (0.962)\n",
            "Epoch: [15][22/38]\t Loss 0.1842 (0.1206)\t Accuracy 0.922 (0.960)\n",
            "Epoch: [15][23/38]\t Loss 0.2242 (0.1249)\t Accuracy 0.919 (0.959)\n",
            "Epoch: [15][24/38]\t Loss 0.1425 (0.1256)\t Accuracy 0.983 (0.959)\n",
            "Epoch: [15][25/38]\t Loss 0.0629 (0.1231)\t Accuracy 0.984 (0.960)\n",
            "Epoch: [15][26/38]\t Loss 0.2012 (0.1260)\t Accuracy 0.919 (0.959)\n",
            "Epoch: [15][27/38]\t Loss 0.2639 (0.1309)\t Accuracy 0.903 (0.957)\n",
            "Epoch: [15][28/38]\t Loss 0.3376 (0.1382)\t Accuracy 0.891 (0.955)\n",
            "Epoch: [15][29/38]\t Loss 0.0693 (0.1358)\t Accuracy 0.984 (0.956)\n",
            "Epoch: [15][30/38]\t Loss 0.1665 (0.1368)\t Accuracy 0.952 (0.956)\n",
            "Epoch: [15][31/38]\t Loss 0.1955 (0.1387)\t Accuracy 0.919 (0.954)\n",
            "Epoch: [15][32/38]\t Loss 0.1498 (0.1390)\t Accuracy 0.952 (0.954)\n",
            "Epoch: [15][33/38]\t Loss 0.1165 (0.1384)\t Accuracy 0.935 (0.954)\n",
            "Epoch: [15][34/38]\t Loss 0.0997 (0.1373)\t Accuracy 0.967 (0.954)\n",
            "Epoch: [15][35/38]\t Loss 0.1429 (0.1374)\t Accuracy 0.938 (0.954)\n",
            "Epoch: [15][36/38]\t Loss 0.0217 (0.1343)\t Accuracy 1.000 (0.955)\n",
            "Epoch: [15][37/38]\t Loss 0.0001 (0.1340)\t Accuracy 1.000 (0.955)\n",
            "Epoch: [15]\t Avg Loss 0.1340\t Avg Accuracy 0.955\n",
            "Test Average Accuracy: 0.7846\n",
            "Epoch: [16][0/38]\t Loss 0.0473 (0.0473)\t Accuracy 0.967 (0.967)\n",
            "Epoch: [16][1/38]\t Loss 0.1055 (0.0771)\t Accuracy 0.968 (0.967)\n",
            "Epoch: [16][2/38]\t Loss 0.1902 (0.1146)\t Accuracy 0.934 (0.957)\n",
            "Epoch: [16][3/38]\t Loss 0.1114 (0.1137)\t Accuracy 0.953 (0.956)\n",
            "Epoch: [16][4/38]\t Loss 0.0265 (0.0959)\t Accuracy 1.000 (0.965)\n",
            "Epoch: [16][5/38]\t Loss 0.1245 (0.1005)\t Accuracy 0.934 (0.960)\n",
            "Epoch: [16][6/38]\t Loss 0.1208 (0.1034)\t Accuracy 0.968 (0.961)\n",
            "Epoch: [16][7/38]\t Loss 0.0305 (0.0943)\t Accuracy 1.000 (0.966)\n",
            "Epoch: [16][8/38]\t Loss 0.0684 (0.0915)\t Accuracy 0.968 (0.966)\n",
            "Epoch: [16][9/38]\t Loss 0.1428 (0.0966)\t Accuracy 0.968 (0.966)\n",
            "Epoch: [16][10/38]\t Loss 0.0590 (0.0932)\t Accuracy 0.984 (0.968)\n",
            "Epoch: [16][11/38]\t Loss 0.0368 (0.0885)\t Accuracy 0.984 (0.969)\n",
            "Epoch: [16][12/38]\t Loss 0.0274 (0.0836)\t Accuracy 1.000 (0.972)\n",
            "Epoch: [16][13/38]\t Loss 0.0748 (0.0830)\t Accuracy 0.984 (0.972)\n",
            "Epoch: [16][14/38]\t Loss 0.1787 (0.0895)\t Accuracy 0.984 (0.973)\n",
            "Epoch: [16][15/38]\t Loss 0.0842 (0.0892)\t Accuracy 0.983 (0.974)\n",
            "Epoch: [16][16/38]\t Loss 0.1466 (0.0925)\t Accuracy 0.951 (0.973)\n",
            "Epoch: [16][17/38]\t Loss 0.0631 (0.0909)\t Accuracy 0.984 (0.973)\n",
            "Epoch: [16][18/38]\t Loss 0.0151 (0.0868)\t Accuracy 1.000 (0.975)\n",
            "Epoch: [16][19/38]\t Loss 0.0101 (0.0829)\t Accuracy 1.000 (0.976)\n",
            "Epoch: [16][20/38]\t Loss 0.0887 (0.0832)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [16][21/38]\t Loss 0.0323 (0.0808)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [16][22/38]\t Loss 0.0685 (0.0803)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [16][23/38]\t Loss 0.1336 (0.0826)\t Accuracy 0.937 (0.975)\n",
            "Epoch: [16][24/38]\t Loss 0.1607 (0.0856)\t Accuracy 0.951 (0.974)\n",
            "Epoch: [16][25/38]\t Loss 0.0602 (0.0846)\t Accuracy 0.968 (0.974)\n",
            "Epoch: [16][26/38]\t Loss 0.0683 (0.0840)\t Accuracy 0.984 (0.974)\n",
            "Epoch: [16][27/38]\t Loss 0.2982 (0.0918)\t Accuracy 0.905 (0.972)\n",
            "Epoch: [16][28/38]\t Loss 0.1249 (0.0929)\t Accuracy 0.953 (0.971)\n",
            "Epoch: [16][29/38]\t Loss 0.1498 (0.0948)\t Accuracy 0.951 (0.971)\n",
            "Epoch: [16][30/38]\t Loss 0.2344 (0.0992)\t Accuracy 0.918 (0.969)\n",
            "Epoch: [16][31/38]\t Loss 0.1465 (0.1007)\t Accuracy 0.952 (0.968)\n",
            "Epoch: [16][32/38]\t Loss 0.0992 (0.1006)\t Accuracy 0.984 (0.969)\n",
            "Epoch: [16][33/38]\t Loss 0.0126 (0.0980)\t Accuracy 1.000 (0.970)\n",
            "Epoch: [16][34/38]\t Loss 0.0123 (0.0956)\t Accuracy 1.000 (0.971)\n",
            "Epoch: [16][35/38]\t Loss 0.1257 (0.0964)\t Accuracy 0.984 (0.971)\n",
            "Epoch: [16][36/38]\t Loss 0.1906 (0.0990)\t Accuracy 0.952 (0.971)\n",
            "Epoch: [16][37/38]\t Loss 0.0002 (0.0988)\t Accuracy 1.000 (0.971)\n",
            "Epoch: [16]\t Avg Loss 0.0988\t Avg Accuracy 0.971\n",
            "Test Average Accuracy: 0.7702\n",
            "Epoch: [17][0/38]\t Loss 0.1261 (0.1261)\t Accuracy 0.937 (0.937)\n",
            "Epoch: [17][1/38]\t Loss 0.1337 (0.1299)\t Accuracy 0.968 (0.952)\n",
            "Epoch: [17][2/38]\t Loss 0.0494 (0.1031)\t Accuracy 0.984 (0.963)\n",
            "Epoch: [17][3/38]\t Loss 0.0773 (0.0968)\t Accuracy 0.967 (0.964)\n",
            "Epoch: [17][4/38]\t Loss 0.0558 (0.0887)\t Accuracy 0.984 (0.968)\n",
            "Epoch: [17][5/38]\t Loss 0.0778 (0.0869)\t Accuracy 0.953 (0.965)\n",
            "Epoch: [17][6/38]\t Loss 0.0473 (0.0811)\t Accuracy 0.984 (0.968)\n",
            "Epoch: [17][7/38]\t Loss 0.0765 (0.0805)\t Accuracy 0.984 (0.970)\n",
            "Epoch: [17][8/38]\t Loss 0.0297 (0.0749)\t Accuracy 1.000 (0.973)\n",
            "Epoch: [17][9/38]\t Loss 0.0225 (0.0697)\t Accuracy 1.000 (0.976)\n",
            "Epoch: [17][10/38]\t Loss 0.0605 (0.0688)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [17][11/38]\t Loss 0.0651 (0.0685)\t Accuracy 0.953 (0.975)\n",
            "Epoch: [17][12/38]\t Loss 0.0989 (0.0709)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [17][13/38]\t Loss 0.1185 (0.0742)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [17][14/38]\t Loss 0.0234 (0.0709)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [17][15/38]\t Loss 0.0663 (0.0706)\t Accuracy 0.967 (0.977)\n",
            "Epoch: [17][16/38]\t Loss 0.0189 (0.0675)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [17][17/38]\t Loss 0.0198 (0.0650)\t Accuracy 1.000 (0.980)\n",
            "Epoch: [17][18/38]\t Loss 0.1031 (0.0670)\t Accuracy 0.952 (0.978)\n",
            "Epoch: [17][19/38]\t Loss 0.1506 (0.0712)\t Accuracy 0.937 (0.976)\n",
            "Epoch: [17][20/38]\t Loss 0.0376 (0.0696)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [17][21/38]\t Loss 0.0586 (0.0691)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [17][22/38]\t Loss 0.0798 (0.0696)\t Accuracy 0.968 (0.976)\n",
            "Epoch: [17][23/38]\t Loss 0.0923 (0.0705)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [17][24/38]\t Loss 0.0642 (0.0703)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [17][25/38]\t Loss 0.1421 (0.0731)\t Accuracy 0.953 (0.976)\n",
            "Epoch: [17][26/38]\t Loss 0.0221 (0.0713)\t Accuracy 0.983 (0.976)\n",
            "Epoch: [17][27/38]\t Loss 0.1064 (0.0725)\t Accuracy 0.950 (0.975)\n",
            "Epoch: [17][28/38]\t Loss 0.0229 (0.0708)\t Accuracy 1.000 (0.976)\n",
            "Epoch: [17][29/38]\t Loss 0.0796 (0.0711)\t Accuracy 0.969 (0.976)\n",
            "Epoch: [17][30/38]\t Loss 0.1094 (0.0724)\t Accuracy 0.921 (0.974)\n",
            "Epoch: [17][31/38]\t Loss 0.0846 (0.0727)\t Accuracy 0.984 (0.974)\n",
            "Epoch: [17][32/38]\t Loss 0.2496 (0.0780)\t Accuracy 0.934 (0.973)\n",
            "Epoch: [17][33/38]\t Loss 0.0716 (0.0778)\t Accuracy 0.983 (0.974)\n",
            "Epoch: [17][34/38]\t Loss 0.1354 (0.0794)\t Accuracy 0.935 (0.972)\n",
            "Epoch: [17][35/38]\t Loss 0.0801 (0.0795)\t Accuracy 0.969 (0.972)\n",
            "Epoch: [17][36/38]\t Loss 0.0672 (0.0791)\t Accuracy 0.984 (0.973)\n",
            "Epoch: [17][37/38]\t Loss 0.0000 (0.0790)\t Accuracy 1.000 (0.973)\n",
            "Epoch: [17]\t Avg Loss 0.0790\t Avg Accuracy 0.973\n",
            "Test Average Accuracy: 0.7708\n",
            "Epoch: [18][0/38]\t Loss 0.0458 (0.0458)\t Accuracy 0.969 (0.969)\n",
            "Epoch: [18][1/38]\t Loss 0.1145 (0.0799)\t Accuracy 0.952 (0.961)\n",
            "Epoch: [18][2/38]\t Loss 0.1723 (0.1102)\t Accuracy 0.903 (0.942)\n",
            "Epoch: [18][3/38]\t Loss 0.0768 (0.1019)\t Accuracy 0.968 (0.948)\n",
            "Epoch: [18][4/38]\t Loss 0.0759 (0.0968)\t Accuracy 0.984 (0.955)\n",
            "Epoch: [18][5/38]\t Loss 0.0475 (0.0885)\t Accuracy 1.000 (0.963)\n",
            "Epoch: [18][6/38]\t Loss 0.0580 (0.0841)\t Accuracy 0.984 (0.966)\n",
            "Epoch: [18][7/38]\t Loss 0.0412 (0.0788)\t Accuracy 0.968 (0.966)\n",
            "Epoch: [18][8/38]\t Loss 0.0162 (0.0718)\t Accuracy 1.000 (0.970)\n",
            "Epoch: [18][9/38]\t Loss 0.1426 (0.0790)\t Accuracy 0.984 (0.971)\n",
            "Epoch: [18][10/38]\t Loss 0.0465 (0.0762)\t Accuracy 0.983 (0.972)\n",
            "Epoch: [18][11/38]\t Loss 0.1006 (0.0783)\t Accuracy 0.952 (0.971)\n",
            "Epoch: [18][12/38]\t Loss 0.2418 (0.0911)\t Accuracy 0.922 (0.967)\n",
            "Epoch: [18][13/38]\t Loss 0.0184 (0.0860)\t Accuracy 1.000 (0.969)\n",
            "Epoch: [18][14/38]\t Loss 0.0027 (0.0806)\t Accuracy 1.000 (0.971)\n",
            "Epoch: [18][15/38]\t Loss 0.1084 (0.0824)\t Accuracy 0.952 (0.970)\n",
            "Epoch: [18][16/38]\t Loss 0.0143 (0.0784)\t Accuracy 1.000 (0.972)\n",
            "Epoch: [18][17/38]\t Loss 0.0407 (0.0763)\t Accuracy 0.968 (0.972)\n",
            "Epoch: [18][18/38]\t Loss 0.0197 (0.0733)\t Accuracy 1.000 (0.973)\n",
            "Epoch: [18][19/38]\t Loss 0.0523 (0.0723)\t Accuracy 0.984 (0.974)\n",
            "Epoch: [18][20/38]\t Loss 0.0275 (0.0702)\t Accuracy 0.983 (0.974)\n",
            "Epoch: [18][21/38]\t Loss 0.1081 (0.0720)\t Accuracy 0.969 (0.974)\n",
            "Epoch: [18][22/38]\t Loss 0.0141 (0.0696)\t Accuracy 1.000 (0.975)\n",
            "Epoch: [18][23/38]\t Loss 0.0517 (0.0688)\t Accuracy 0.984 (0.975)\n",
            "Epoch: [18][24/38]\t Loss 0.1171 (0.0708)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [18][25/38]\t Loss 0.1096 (0.0723)\t Accuracy 0.968 (0.975)\n",
            "Epoch: [18][26/38]\t Loss 0.0723 (0.0723)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [18][27/38]\t Loss 0.0777 (0.0725)\t Accuracy 0.969 (0.975)\n",
            "Epoch: [18][28/38]\t Loss 0.0485 (0.0716)\t Accuracy 0.968 (0.975)\n",
            "Epoch: [18][29/38]\t Loss 0.1150 (0.0731)\t Accuracy 0.969 (0.975)\n",
            "Epoch: [18][30/38]\t Loss 0.0403 (0.0721)\t Accuracy 0.984 (0.975)\n",
            "Epoch: [18][31/38]\t Loss 0.1026 (0.0730)\t Accuracy 0.952 (0.974)\n",
            "Epoch: [18][32/38]\t Loss 0.0186 (0.0713)\t Accuracy 1.000 (0.975)\n",
            "Epoch: [18][33/38]\t Loss 0.1146 (0.0725)\t Accuracy 0.949 (0.975)\n",
            "Epoch: [18][34/38]\t Loss 0.1206 (0.0739)\t Accuracy 0.967 (0.974)\n",
            "Epoch: [18][35/38]\t Loss 0.0354 (0.0728)\t Accuracy 0.984 (0.975)\n",
            "Epoch: [18][36/38]\t Loss 0.0660 (0.0726)\t Accuracy 0.935 (0.974)\n",
            "Epoch: [18][37/38]\t Loss 0.2808 (0.0730)\t Accuracy 0.800 (0.973)\n",
            "Epoch: [18]\t Avg Loss 0.0730\t Avg Accuracy 0.973\n",
            "Test Average Accuracy: 0.7892\n",
            "Saving Model...\n",
            "Epoch: [19][0/38]\t Loss 0.0866 (0.0866)\t Accuracy 0.968 (0.968)\n",
            "Epoch: [19][1/38]\t Loss 0.0335 (0.0596)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [19][2/38]\t Loss 0.1499 (0.0894)\t Accuracy 0.952 (0.968)\n",
            "Epoch: [19][3/38]\t Loss 0.0504 (0.0797)\t Accuracy 0.984 (0.972)\n",
            "Epoch: [19][4/38]\t Loss 0.0505 (0.0739)\t Accuracy 0.984 (0.974)\n",
            "Epoch: [19][5/38]\t Loss 0.0636 (0.0722)\t Accuracy 0.984 (0.976)\n",
            "Epoch: [19][6/38]\t Loss 0.0480 (0.0687)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [19][7/38]\t Loss 0.0212 (0.0628)\t Accuracy 0.984 (0.978)\n",
            "Epoch: [19][8/38]\t Loss 0.0267 (0.0588)\t Accuracy 1.000 (0.980)\n",
            "Epoch: [19][9/38]\t Loss 0.0386 (0.0567)\t Accuracy 0.984 (0.981)\n",
            "Epoch: [19][10/38]\t Loss 0.0707 (0.0580)\t Accuracy 0.969 (0.980)\n",
            "Epoch: [19][11/38]\t Loss 0.0020 (0.0534)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [19][12/38]\t Loss 0.1360 (0.0597)\t Accuracy 0.935 (0.978)\n",
            "Epoch: [19][13/38]\t Loss 0.0077 (0.0560)\t Accuracy 1.000 (0.979)\n",
            "Epoch: [19][14/38]\t Loss 0.0863 (0.0581)\t Accuracy 0.984 (0.980)\n",
            "Epoch: [19][15/38]\t Loss 0.0190 (0.0556)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [19][16/38]\t Loss 0.1528 (0.0613)\t Accuracy 0.952 (0.979)\n",
            "Epoch: [19][17/38]\t Loss 0.2451 (0.0718)\t Accuracy 0.922 (0.976)\n",
            "Epoch: [19][18/38]\t Loss 0.0975 (0.0731)\t Accuracy 0.967 (0.976)\n",
            "Epoch: [19][19/38]\t Loss 0.0866 (0.0738)\t Accuracy 0.968 (0.975)\n",
            "Epoch: [19][20/38]\t Loss 0.1543 (0.0775)\t Accuracy 0.967 (0.975)\n",
            "Epoch: [19][21/38]\t Loss 0.0273 (0.0752)\t Accuracy 0.984 (0.975)\n",
            "Epoch: [19][22/38]\t Loss 0.1186 (0.0771)\t Accuracy 0.952 (0.974)\n",
            "Epoch: [19][23/38]\t Loss 0.0324 (0.0753)\t Accuracy 1.000 (0.975)\n",
            "Epoch: [19][24/38]\t Loss 0.0400 (0.0739)\t Accuracy 0.983 (0.976)\n",
            "Epoch: [19][25/38]\t Loss 0.0095 (0.0714)\t Accuracy 1.000 (0.977)\n",
            "Epoch: [19][26/38]\t Loss 0.0533 (0.0707)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [19][27/38]\t Loss 0.0238 (0.0691)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [19][28/38]\t Loss 0.0478 (0.0683)\t Accuracy 0.967 (0.977)\n",
            "Epoch: [19][29/38]\t Loss 0.0659 (0.0683)\t Accuracy 0.968 (0.977)\n",
            "Epoch: [19][30/38]\t Loss 0.0175 (0.0666)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [19][31/38]\t Loss 0.0767 (0.0669)\t Accuracy 0.952 (0.977)\n",
            "Epoch: [19][32/38]\t Loss 0.0220 (0.0655)\t Accuracy 0.984 (0.977)\n",
            "Epoch: [19][33/38]\t Loss 0.0114 (0.0639)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [19][34/38]\t Loss 0.0260 (0.0628)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [19][35/38]\t Loss 0.0264 (0.0618)\t Accuracy 1.000 (0.979)\n",
            "Epoch: [19][36/38]\t Loss 0.1567 (0.0643)\t Accuracy 0.951 (0.978)\n",
            "Epoch: [19][37/38]\t Loss 0.0013 (0.0642)\t Accuracy 1.000 (0.978)\n",
            "Epoch: [19]\t Avg Loss 0.0642\t Avg Accuracy 0.978\n",
            "Test Average Accuracy: 0.7768\n",
            "Epoch: [20][0/38]\t Loss 0.0763 (0.0763)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [20][1/38]\t Loss 0.0300 (0.0532)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [20][2/38]\t Loss 0.1208 (0.0757)\t Accuracy 0.937 (0.968)\n",
            "Epoch: [20][3/38]\t Loss 0.0515 (0.0698)\t Accuracy 0.984 (0.972)\n",
            "Epoch: [20][4/38]\t Loss 0.0323 (0.0623)\t Accuracy 0.984 (0.974)\n",
            "Epoch: [20][5/38]\t Loss 0.0116 (0.0537)\t Accuracy 1.000 (0.979)\n",
            "Epoch: [20][6/38]\t Loss 0.0262 (0.0499)\t Accuracy 0.983 (0.979)\n",
            "Epoch: [20][7/38]\t Loss 0.0044 (0.0443)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [20][8/38]\t Loss 0.0932 (0.0498)\t Accuracy 0.969 (0.980)\n",
            "Epoch: [20][9/38]\t Loss 0.0521 (0.0501)\t Accuracy 0.968 (0.979)\n",
            "Epoch: [20][10/38]\t Loss 0.0138 (0.0467)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [20][11/38]\t Loss 0.0085 (0.0436)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [20][12/38]\t Loss 0.0285 (0.0424)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][13/38]\t Loss 0.0099 (0.0401)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [20][14/38]\t Loss 0.0952 (0.0438)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [20][15/38]\t Loss 0.1437 (0.0499)\t Accuracy 0.967 (0.983)\n",
            "Epoch: [20][16/38]\t Loss 0.0588 (0.0504)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [20][17/38]\t Loss 0.0044 (0.0479)\t Accuracy 1.000 (0.985)\n",
            "Epoch: [20][18/38]\t Loss 0.0408 (0.0475)\t Accuracy 0.984 (0.985)\n",
            "Epoch: [20][19/38]\t Loss 0.0278 (0.0465)\t Accuracy 0.984 (0.985)\n",
            "Epoch: [20][20/38]\t Loss 0.1027 (0.0493)\t Accuracy 0.953 (0.983)\n",
            "Epoch: [20][21/38]\t Loss 0.0690 (0.0502)\t Accuracy 0.968 (0.983)\n",
            "Epoch: [20][22/38]\t Loss 0.0066 (0.0483)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [20][23/38]\t Loss 0.0271 (0.0474)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][24/38]\t Loss 0.0218 (0.0464)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [20][25/38]\t Loss 0.0427 (0.0463)\t Accuracy 0.968 (0.983)\n",
            "Epoch: [20][26/38]\t Loss 0.1651 (0.0507)\t Accuracy 0.935 (0.982)\n",
            "Epoch: [20][27/38]\t Loss 0.0138 (0.0493)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [20][28/38]\t Loss 0.0059 (0.0478)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [20][29/38]\t Loss 0.0156 (0.0468)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [20][30/38]\t Loss 0.0278 (0.0462)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [20][31/38]\t Loss 0.0588 (0.0466)\t Accuracy 0.953 (0.983)\n",
            "Epoch: [20][32/38]\t Loss 0.0383 (0.0463)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][33/38]\t Loss 0.0369 (0.0460)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][34/38]\t Loss 0.0220 (0.0454)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][35/38]\t Loss 0.0783 (0.0463)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [20][36/38]\t Loss 0.0655 (0.0468)\t Accuracy 0.968 (0.983)\n",
            "Epoch: [20][37/38]\t Loss 0.0000 (0.0467)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [20]\t Avg Loss 0.0467\t Avg Accuracy 0.983\n",
            "Test Average Accuracy: 0.7781\n",
            "Epoch: [21][0/38]\t Loss 0.0078 (0.0078)\t Accuracy 1.000 (1.000)\n",
            "Epoch: [21][1/38]\t Loss 0.0107 (0.0093)\t Accuracy 1.000 (1.000)\n",
            "Epoch: [21][2/38]\t Loss 0.0227 (0.0139)\t Accuracy 1.000 (1.000)\n",
            "Epoch: [21][3/38]\t Loss 0.0567 (0.0244)\t Accuracy 0.984 (0.996)\n",
            "Epoch: [21][4/38]\t Loss 0.0401 (0.0275)\t Accuracy 0.983 (0.994)\n",
            "Epoch: [21][5/38]\t Loss 0.0375 (0.0291)\t Accuracy 0.984 (0.992)\n",
            "Epoch: [21][6/38]\t Loss 0.0394 (0.0306)\t Accuracy 0.984 (0.991)\n",
            "Epoch: [21][7/38]\t Loss 0.1219 (0.0423)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [21][8/38]\t Loss 0.1191 (0.0508)\t Accuracy 0.968 (0.987)\n",
            "Epoch: [21][9/38]\t Loss 0.0967 (0.0554)\t Accuracy 0.984 (0.987)\n",
            "Epoch: [21][10/38]\t Loss 0.0177 (0.0519)\t Accuracy 0.984 (0.987)\n",
            "Epoch: [21][11/38]\t Loss 0.0268 (0.0498)\t Accuracy 0.984 (0.987)\n",
            "Epoch: [21][12/38]\t Loss 0.0147 (0.0471)\t Accuracy 1.000 (0.988)\n",
            "Epoch: [21][13/38]\t Loss 0.1128 (0.0517)\t Accuracy 0.968 (0.986)\n",
            "Epoch: [21][14/38]\t Loss 0.0434 (0.0512)\t Accuracy 0.984 (0.986)\n",
            "Epoch: [21][15/38]\t Loss 0.0159 (0.0490)\t Accuracy 1.000 (0.987)\n",
            "Epoch: [21][16/38]\t Loss 0.1692 (0.0561)\t Accuracy 0.984 (0.987)\n",
            "Epoch: [21][17/38]\t Loss 0.0016 (0.0530)\t Accuracy 1.000 (0.987)\n",
            "Epoch: [21][18/38]\t Loss 0.0192 (0.0512)\t Accuracy 1.000 (0.988)\n",
            "Epoch: [21][19/38]\t Loss 0.0096 (0.0491)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [21][20/38]\t Loss 0.0056 (0.0470)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [21][21/38]\t Loss 0.0242 (0.0460)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [21][22/38]\t Loss 0.1364 (0.0499)\t Accuracy 0.968 (0.989)\n",
            "Epoch: [21][23/38]\t Loss 0.0410 (0.0496)\t Accuracy 0.984 (0.989)\n",
            "Epoch: [21][24/38]\t Loss 0.0330 (0.0489)\t Accuracy 0.984 (0.988)\n",
            "Epoch: [21][25/38]\t Loss 0.0155 (0.0476)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [21][26/38]\t Loss 0.0017 (0.0459)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [21][27/38]\t Loss 0.0134 (0.0448)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [21][28/38]\t Loss 0.0050 (0.0434)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [21][29/38]\t Loss 0.0304 (0.0430)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [21][30/38]\t Loss 0.0477 (0.0431)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [21][31/38]\t Loss 0.0192 (0.0424)\t Accuracy 0.984 (0.989)\n",
            "Epoch: [21][32/38]\t Loss 0.1393 (0.0452)\t Accuracy 0.984 (0.989)\n",
            "Epoch: [21][33/38]\t Loss 0.0733 (0.0461)\t Accuracy 0.952 (0.988)\n",
            "Epoch: [21][34/38]\t Loss 0.0641 (0.0466)\t Accuracy 0.968 (0.988)\n",
            "Epoch: [21][35/38]\t Loss 0.0017 (0.0453)\t Accuracy 1.000 (0.988)\n",
            "Epoch: [21][36/38]\t Loss 0.0652 (0.0459)\t Accuracy 0.968 (0.987)\n",
            "Epoch: [21][37/38]\t Loss 0.0000 (0.0458)\t Accuracy 1.000 (0.987)\n",
            "Epoch: [21]\t Avg Loss 0.0458\t Avg Accuracy 0.987\n",
            "Test Average Accuracy: 0.7807\n",
            "Epoch: [22][0/38]\t Loss 0.0922 (0.0922)\t Accuracy 0.952 (0.952)\n",
            "Epoch: [22][1/38]\t Loss 0.0982 (0.0952)\t Accuracy 0.984 (0.969)\n",
            "Epoch: [22][2/38]\t Loss 0.0035 (0.0658)\t Accuracy 1.000 (0.979)\n",
            "Epoch: [22][3/38]\t Loss 0.0265 (0.0560)\t Accuracy 0.984 (0.980)\n",
            "Epoch: [22][4/38]\t Loss 0.0665 (0.0581)\t Accuracy 0.968 (0.977)\n",
            "Epoch: [22][5/38]\t Loss 0.0028 (0.0487)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [22][6/38]\t Loss 0.0046 (0.0425)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22][7/38]\t Loss 0.0299 (0.0409)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [22][8/38]\t Loss 0.0342 (0.0402)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [22][9/38]\t Loss 0.0766 (0.0438)\t Accuracy 0.968 (0.982)\n",
            "Epoch: [22][10/38]\t Loss 0.0044 (0.0402)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22][11/38]\t Loss 0.0468 (0.0408)\t Accuracy 0.968 (0.983)\n",
            "Epoch: [22][12/38]\t Loss 0.0428 (0.0409)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [22][13/38]\t Loss 0.1094 (0.0459)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [22][14/38]\t Loss 0.0102 (0.0434)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [22][15/38]\t Loss 0.0072 (0.0411)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [22][16/38]\t Loss 0.0200 (0.0398)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [22][17/38]\t Loss 0.1074 (0.0436)\t Accuracy 0.968 (0.982)\n",
            "Epoch: [22][18/38]\t Loss 0.0050 (0.0415)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [22][19/38]\t Loss 0.1530 (0.0471)\t Accuracy 0.952 (0.982)\n",
            "Epoch: [22][20/38]\t Loss 0.1485 (0.0519)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [22][21/38]\t Loss 0.0275 (0.0508)\t Accuracy 0.984 (0.981)\n",
            "Epoch: [22][22/38]\t Loss 0.2193 (0.0578)\t Accuracy 0.967 (0.981)\n",
            "Epoch: [22][23/38]\t Loss 0.1164 (0.0602)\t Accuracy 0.967 (0.980)\n",
            "Epoch: [22][24/38]\t Loss 0.0341 (0.0591)\t Accuracy 0.984 (0.980)\n",
            "Epoch: [22][25/38]\t Loss 0.0090 (0.0572)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [22][26/38]\t Loss 0.0191 (0.0558)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [22][27/38]\t Loss 0.0175 (0.0544)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [22][28/38]\t Loss 0.0019 (0.0526)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [22][29/38]\t Loss 0.0451 (0.0524)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [22][30/38]\t Loss 0.0022 (0.0508)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [22][31/38]\t Loss 0.0226 (0.0499)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22][32/38]\t Loss 0.0101 (0.0487)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22][33/38]\t Loss 0.0495 (0.0487)\t Accuracy 0.968 (0.984)\n",
            "Epoch: [22][34/38]\t Loss 0.0521 (0.0488)\t Accuracy 0.968 (0.984)\n",
            "Epoch: [22][35/38]\t Loss 0.0263 (0.0482)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22][36/38]\t Loss 0.0710 (0.0488)\t Accuracy 0.966 (0.984)\n",
            "Epoch: [22][37/38]\t Loss 0.0035 (0.0487)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [22]\t Avg Loss 0.0487\t Avg Accuracy 0.984\n",
            "Test Average Accuracy: 0.7846\n",
            "Epoch: [23][0/38]\t Loss 0.0009 (0.0009)\t Accuracy 1.000 (1.000)\n",
            "Epoch: [23][1/38]\t Loss 0.0149 (0.0078)\t Accuracy 1.000 (1.000)\n",
            "Epoch: [23][2/38]\t Loss 0.0673 (0.0276)\t Accuracy 0.984 (0.995)\n",
            "Epoch: [23][3/38]\t Loss 0.0103 (0.0233)\t Accuracy 1.000 (0.996)\n",
            "Epoch: [23][4/38]\t Loss 0.0082 (0.0203)\t Accuracy 1.000 (0.997)\n",
            "Epoch: [23][5/38]\t Loss 0.0151 (0.0194)\t Accuracy 1.000 (0.997)\n",
            "Epoch: [23][6/38]\t Loss 0.0042 (0.0172)\t Accuracy 1.000 (0.998)\n",
            "Epoch: [23][7/38]\t Loss 0.0009 (0.0151)\t Accuracy 1.000 (0.998)\n",
            "Epoch: [23][8/38]\t Loss 0.0320 (0.0170)\t Accuracy 0.984 (0.996)\n",
            "Epoch: [23][9/38]\t Loss 0.0041 (0.0157)\t Accuracy 1.000 (0.997)\n",
            "Epoch: [23][10/38]\t Loss 0.0244 (0.0165)\t Accuracy 0.984 (0.996)\n",
            "Epoch: [23][11/38]\t Loss 0.0210 (0.0169)\t Accuracy 1.000 (0.996)\n",
            "Epoch: [23][12/38]\t Loss 0.1869 (0.0298)\t Accuracy 0.952 (0.993)\n",
            "Epoch: [23][13/38]\t Loss 0.1010 (0.0347)\t Accuracy 0.967 (0.991)\n",
            "Epoch: [23][14/38]\t Loss 0.0352 (0.0347)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][15/38]\t Loss 0.0368 (0.0349)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][16/38]\t Loss 0.0127 (0.0336)\t Accuracy 1.000 (0.991)\n",
            "Epoch: [23][17/38]\t Loss 0.0451 (0.0342)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][18/38]\t Loss 0.0564 (0.0354)\t Accuracy 0.967 (0.989)\n",
            "Epoch: [23][19/38]\t Loss 0.0091 (0.0340)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][20/38]\t Loss 0.0127 (0.0330)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][21/38]\t Loss 0.0034 (0.0317)\t Accuracy 1.000 (0.991)\n",
            "Epoch: [23][22/38]\t Loss 0.0353 (0.0319)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][23/38]\t Loss 0.0066 (0.0308)\t Accuracy 1.000 (0.991)\n",
            "Epoch: [23][24/38]\t Loss 0.0323 (0.0309)\t Accuracy 0.968 (0.990)\n",
            "Epoch: [23][25/38]\t Loss 0.0662 (0.0323)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][26/38]\t Loss 0.0018 (0.0311)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][27/38]\t Loss 0.0377 (0.0313)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][28/38]\t Loss 0.0154 (0.0308)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][29/38]\t Loss 0.1199 (0.0337)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][30/38]\t Loss 0.0443 (0.0341)\t Accuracy 0.984 (0.990)\n",
            "Epoch: [23][31/38]\t Loss 0.0067 (0.0333)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][32/38]\t Loss 0.1186 (0.0359)\t Accuracy 0.969 (0.989)\n",
            "Epoch: [23][33/38]\t Loss 0.0023 (0.0349)\t Accuracy 1.000 (0.990)\n",
            "Epoch: [23][34/38]\t Loss 0.0497 (0.0353)\t Accuracy 0.967 (0.989)\n",
            "Epoch: [23][35/38]\t Loss 0.0044 (0.0345)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [23][36/38]\t Loss 0.0354 (0.0345)\t Accuracy 0.984 (0.989)\n",
            "Epoch: [23][37/38]\t Loss 0.0000 (0.0345)\t Accuracy 1.000 (0.989)\n",
            "Epoch: [23]\t Avg Loss 0.0345\t Avg Accuracy 0.989\n",
            "Test Average Accuracy: 0.7768\n",
            "Epoch: [24][0/38]\t Loss 0.0660 (0.0660)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [24][1/38]\t Loss 0.0026 (0.0348)\t Accuracy 1.000 (0.992)\n",
            "Epoch: [24][2/38]\t Loss 0.0474 (0.0389)\t Accuracy 0.984 (0.989)\n",
            "Epoch: [24][3/38]\t Loss 0.0505 (0.0419)\t Accuracy 0.969 (0.984)\n",
            "Epoch: [24][4/38]\t Loss 0.0410 (0.0417)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [24][5/38]\t Loss 0.0227 (0.0386)\t Accuracy 0.983 (0.981)\n",
            "Epoch: [24][6/38]\t Loss 0.1409 (0.0536)\t Accuracy 0.953 (0.977)\n",
            "Epoch: [24][7/38]\t Loss 0.0512 (0.0533)\t Accuracy 0.984 (0.978)\n",
            "Epoch: [24][8/38]\t Loss 0.0273 (0.0504)\t Accuracy 0.984 (0.979)\n",
            "Epoch: [24][9/38]\t Loss 0.0748 (0.0528)\t Accuracy 0.984 (0.979)\n",
            "Epoch: [24][10/38]\t Loss 0.0203 (0.0498)\t Accuracy 0.984 (0.980)\n",
            "Epoch: [24][11/38]\t Loss 0.0021 (0.0458)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [24][12/38]\t Loss 0.0006 (0.0424)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [24][13/38]\t Loss 0.0485 (0.0428)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [24][14/38]\t Loss 0.1434 (0.0493)\t Accuracy 0.933 (0.980)\n",
            "Epoch: [24][15/38]\t Loss 0.0076 (0.0466)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [24][16/38]\t Loss 0.1436 (0.0525)\t Accuracy 0.969 (0.980)\n",
            "Epoch: [24][17/38]\t Loss 0.0023 (0.0497)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [24][18/38]\t Loss 0.0978 (0.0521)\t Accuracy 0.949 (0.980)\n",
            "Epoch: [24][19/38]\t Loss 0.0067 (0.0499)\t Accuracy 1.000 (0.981)\n",
            "Epoch: [24][20/38]\t Loss 0.0039 (0.0477)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [24][21/38]\t Loss 0.0164 (0.0463)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [24][22/38]\t Loss 0.0214 (0.0452)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [24][23/38]\t Loss 0.1149 (0.0482)\t Accuracy 0.969 (0.982)\n",
            "Epoch: [24][24/38]\t Loss 0.0456 (0.0481)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [24][25/38]\t Loss 0.0302 (0.0474)\t Accuracy 0.968 (0.981)\n",
            "Epoch: [24][26/38]\t Loss 0.0250 (0.0466)\t Accuracy 0.984 (0.981)\n",
            "Epoch: [24][27/38]\t Loss 0.0096 (0.0452)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [24][28/38]\t Loss 0.0173 (0.0443)\t Accuracy 1.000 (0.982)\n",
            "Epoch: [24][29/38]\t Loss 0.0007 (0.0428)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [24][30/38]\t Loss 0.0188 (0.0420)\t Accuracy 0.984 (0.983)\n",
            "Epoch: [24][31/38]\t Loss 0.0123 (0.0411)\t Accuracy 1.000 (0.983)\n",
            "Epoch: [24][32/38]\t Loss 0.0562 (0.0415)\t Accuracy 0.983 (0.983)\n",
            "Epoch: [24][33/38]\t Loss 0.0053 (0.0404)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [24][34/38]\t Loss 0.0046 (0.0394)\t Accuracy 1.000 (0.984)\n",
            "Epoch: [24][35/38]\t Loss 0.0236 (0.0390)\t Accuracy 0.984 (0.984)\n",
            "Epoch: [24][36/38]\t Loss 0.0085 (0.0381)\t Accuracy 1.000 (0.985)\n",
            "Epoch: [24][37/38]\t Loss 0.0001 (0.0381)\t Accuracy 1.000 (0.985)\n",
            "Epoch: [24]\t Avg Loss 0.0381\t Avg Accuracy 0.985\n",
            "Test Average Accuracy: 0.7702\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "  \"batch_size\": 64,\n",
        "    \"num_epochs\": 25,\n",
        "    \"lr\": 3e-3,\n",
        "    \"max_grad_norm\": 5.0,\n",
        "    \"embed_dim\": 100,\n",
        "    \"word_gru_hidden_dim\": 100,\n",
        "    \"sent_gru_hidden_dim\": 100,\n",
        "    \"word_gru_num_layers\": 1,\n",
        "    \"sent_gru_num_layers\": 1,\n",
        "    \"word_att_dim\": 200,\n",
        "    \"sent_att_dim\": 200,\n",
        "    \"vocab_path\": \"glove.6B.100d.txt\",\n",
        "    \"pretrain\": True,\n",
        "    \"freeze\": False,\n",
        "    \"use_layer_norm\": True,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXwZ8dEj2NaA"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_weights(corpus_vocab, embed_dim, device):\n",
        "  save_dir = os.path.join('glove_pretrained.pt')\n",
        "  if os.path.exists(save_dir):\n",
        "    return torch.load(save_dir, map_location=device)\n",
        "\n",
        "  corpus_set = set(corpus_vocab)\n",
        "  pretrained_vocab = set()\n",
        "  glove_pretrained = torch.zeros(len(corpus_vocab), embed_dim)\n",
        "  with open(os.path.join('glove.6B.100d.txt'), 'rb') as f:\n",
        "    for l in tqdm(f):\n",
        "      line = l.decode().split()\n",
        "      if line[0] in corpus_set:\n",
        "        pretrained_vocab.add(line[0])\n",
        "        glove_pretrained[corpus_vocab.index(line[0])] = torch.from_numpy(np.array(line[1:]).astype(float))\n",
        "\n",
        "    var = float(torch.var(glove_pretrained))\n",
        "    for oov in corpus_set.difference(pretrained_vocab):\n",
        "      glove_pretrained[corpus_vocab.index(oov)]  = torch.empty(100).float().uniform_(-var, var)\n",
        "    print('weight size: ', glove_pretrained.size())\n",
        "    torch.save(glove_pretrained, save_dir)\n",
        "  return glove_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRuP0MzY6Gjc"
      },
      "outputs": [],
      "source": [
        "def map_sentence_to_color(words, scores, sent_score):\n",
        "  sentencemap = matplotlib.cm.get_cmap('binary')\n",
        "  wordmap = matplotlib.cm.get_cmap('OrRd')\n",
        "  result = '<p><span style=\"margin:5px; padding:5px; background-color: {}\">'\\\n",
        "    .format(matplotlib.colors.rgb2hex(sentencemap(sent_score)[:3]))\n",
        "  template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
        "  for word, score in zip(words, scores):\n",
        "    color = matplotlib.colors.rgb2hex(wordmap(scores)[:3])\n",
        "    result += template.format(color, '&nbsp' + word + '&nbsp')\n",
        "  result += '</span><p>'\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QQQscl98pnK"
      },
      "outputs": [],
      "source": [
        "def bar_chart(categories, scores,graph_title='Prediction', output_name='prediction_bar_char.pmg'):\n",
        "  y_pos = arange(len(categories))\n",
        "  plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
        "  plt.xticks(y_pos, categories)\n",
        "  plt.ylabel('Attention Score')\n",
        "  plt.title(graph_title)\n",
        "\n",
        "  plt.gca().spines['top'].set_visible(False)\n",
        "  plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "  plt.savefig(output_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BAqTVND9bzX"
      },
      "outputs": [],
      "source": [
        "def visualize(model, dataset, doc):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  orig_doc = [word_tokenize(sent) for sent in sent_tokenize(doc)]\n",
        "  doc, num_sents, num_words = dataset.transform(doc)\n",
        "  label = 0\n",
        "\n",
        "  doc, label, doc_length, sent_length = collate_fn([(doc, label, num_sents, num_words)])\n",
        "  score, word_att_weight, sentence_att_weight = model(doc.to(device), doc_length.to(device), sent_length.to(device))\n",
        "\n",
        "  classes = ['Cryptography', 'Electronics', 'Medical', 'Space']\n",
        "  result = \"<h2>Attention Visualization</h2>\"\n",
        "\n",
        "  bar_chart(classes, torch.softmax(score.detach(), dim=1).flatten().cpu(), 'Prediction')\n",
        "  result += '<br><img src=\"prediction_bar_chart.png\"><br>'\n",
        "  for orig_sent, att_weight, sent_weight in zip(orig_doc, word_att_weight[0].tolist(), sentence_att_weight[0].tolist()):\n",
        "    result += map_sentence_to_color(orig_sent, att_weight, sent_weight)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PdBRJtsLCdaK"
      },
      "outputs": [],
      "source": [
        "import webbrowser\n",
        "checkpoint = torch.load('best_model/model.pth.tar')\n",
        "model = checkpoint['model']\n",
        "model.eval()\n",
        "\n",
        "dataset = News20Dataset('glove.6B.100d.txt', is_train=False)\n",
        "doc = \"Amidst this cosmic symphony, instruments of perception extend our senses beyond the limitations of our mortal coil. We gaze through lenses of glass and metal, peering into realms unseen, where galaxies spiral in cosmic embrace. Signals, like ethereal messengers, traverse the void, carrying secrets encoded in the language of pulses and waves.\"\n",
        "result = visualize(model, dataset, doc)\n",
        "with open('result.html', 'w') as f:\n",
        "  f.write(result)\n",
        "\n",
        "webbrowser.open('file://'+os.path.realpath('result.html'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aQpZtHNMChj9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "with open('result.html', 'r') as file:\n",
        "  html_content = file.read()\n",
        "  display(HTML(file.read()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPd5EBIcPqd+b1vbh2gRl99",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}